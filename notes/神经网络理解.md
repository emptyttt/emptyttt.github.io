# ç¥ç»ç½‘ç»œè¯†åˆ«æ‰‹å†™å­—

<img src="C:\Users\Anna\AppData\Roaming\Typora\typora-user-images\image-20250410124813728.png" alt="image-20250410124813728" style="zoom: 33%;" />

Sigmoid

![image-20250410125336694](C:\Users\Anna\AppData\Roaming\Typora\typora-user-images\image-20250410125336694.png)

ç†è§£æƒé‡å’Œbias

![image-20250410125914203](C:\Users\Anna\AppData\Roaming\Typora\typora-user-images\image-20250410125914203.png)

ç†è§£çŸ©é˜µä¹˜æ³•

![image-20250410130311113](C:\Users\Anna\AppData\Roaming\Typora\typora-user-images\image-20250410130311113.png)

![image-20250410130607538](C:\Users\Anna\AppData\Roaming\Typora\typora-user-images\image-20250410130607538.png)

![image-20250410130622816](C:\Users\Anna\AppData\Roaming\Typora\typora-user-images\image-20250410130622816.png)

![image-20250410130839427](C:\Users\Anna\AppData\Roaming\Typora\typora-user-images\image-20250410130839427.png)

# ä½œä¸š-è¯»è®ºæ–‡

â€¢AlexNet: The first deep network
https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
â€¢VGG: Classical deep network
https://arxiv.org/pdf/1409.1556.pdf
â€¢ResNet: Classical deep network
https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf

Network Component

1.Please summarize the advantages and disadvantages of convolutional layers and fully-connected layers.

 convolutional layers  can catch the multi-dimensions' features,can accept any size input but is not good at classification,fully-connected layers is good at classification ,but can only accept one dimension input

improvement

**convolutional layers**

> [!NOTE]
>
> **Advantages:**
>
> Capture spatial and local features effectively
>
> Accept varible-sized input
>
> **Parameter sharing reduces the number of parameters**?
>
> Effective for feature extraction from image and Sequences
>
> **disadvantages:**
>
> not suitable for final classification  tasks alone
>
> more complex to design and tune

 fully-connected layers

> [!NOTE]
>
> **Advantages:**
>
> Powerful for learning complex pattern and final classification
>
> Every neuron is connected to all previous outputs, allowing global information integration
>
> **disadvantages:**
>
> Require fixed-sized 1D input
>
> Large number of parameters leads to high memory and compution cost
>
> Prone to overfitting if not regularized properly

2.Why do VGG and ResNet use 3x3 convolutions, instead of 7x7 convolutions as in AlexNet?

Because 3 3*3 convolution layers have the same Receptive Field as one layer of 7x7 convolution ï¼Œat the same time,the beforer has less parameters than the later( **reducing memory and overfitting risk.**).Also,the beforer has 3 nonlinear ,which is better than the later(**enhancing the modelâ€™s representational power compared to only one in the 7Ã—7 case**.),which only has one nonlinear.

- 1Ã— 7Ã—7 conv with C channels: 49CÂ² params
- 3Ã— 3Ã—3 convs: 27CÂ² params

> [!NOTE]
>
> **Answer:**
>
> 1. **Same Receptive Field**:
>     Stacking three 3Ã—3 convolutional layers gives the same receptive field (7Ã—7) as a single 7Ã—7 convolution layer.
> 2. **Fewer Parameters**:
>     Multiple 3Ã—3 layers use significantly fewer parameters **compared to** a single 7Ã—7 layer, **reducing memory and overfitting risk.**
>    - 1Ã— 7Ã—7 conv with C channels: 49CÂ² params
>    - 3Ã— 3Ã—3 convs: 27CÂ² params
> 3. **More Nonlinearities**:
>     Each 3Ã—3 convolution is followed by a non-linear activation (e.g., ReLU). So stacking three layers introduces three nonlinearities, enhancing the modelâ€™s representational power compared to only one in the 7Ã—7 case.
> 4. **Better Feature Abstraction**:?
>     Multiple smaller convolutions allow for more complex hierarchical feature extraction.

3.Why do VGG and ResNet divide their networks into 5 blocks?

> [!NOTE]
>
> 1.åˆ†å±‚ç‰¹å¾æå–
>
> Deep networks naturally integrate low/mid/highlevel features  and classifiers in an end-to-end multilayer fashion, and the â€œlevelsâ€ of features can be enriched by the number of stacked layers (depth).ã€**Hierarchical feature extraction**The first few modules extract low - level features such as edges and textures, while the latter modules extract high - level abstract features such as semantics and categories, conforming to the hierarchical structure of human visual cognition.ã€‘
>
> 2.**Modular design**ï¼ˆæ¨¡å—åŒ–è®¾è®¡ä¾¿äºæ‰©å±•ï¼‰
>
> By repeatedly stacking modules composed of convolutional layers and pooling layers, the network structure becomes simpler and more scalable, making it easy to extract features at different levels by increasing the depth of modules.

4**.Why generate feature maps on different scales (sizes) in different blocks?**

**Core Reason: Complementarity and Efficiency of Multi - Scale Features**

- Feature maps of different scales can be fused in tasks like detection and segmentation:
  - Smaller feature maps (rich in semantic information) are used to identify object categories;
  - Larger feature maps (rich in spatial information) are used to localize object positions.

5.Can we use 1x1 convolutional layers to replace fully-connected layers?

yesã€‚but we finally get a class score map with the number of channels equal to the number of classes,a variable spatial resolution, dependent on the input image size. Finally, to obtain a fixed-size vector of class scores for the image, the class score map is spatially averaged (sum-pooled). 

> [!NOTE]
>
> **Q: Can we use 1Ã—1 convolutional layers to replace fully-connected layers?**
>  **A: Yes, we can, especially in fully convolutional networks (FCNs).**
>
> 1. **How it works:**
>     A 1Ã—1 convolution acts like a fully-connected layer applied to each spatial location independently, transforming channel information without changing spatial dimensions.
> 2. **Output:**
>     If the number of output channels equals the number of classes, the network produces a **class score map** â€” each location has a score for each class.
> 3. **Final classification:**
>     To convert this variable-size score map into a fixed-length vector for classification, **global average pooling** (or sum-pooling) is applied to produce a single score per class.
> 4. **When is this useful?**
>    - When using **fully convolutional architectures** (e.g., in semantic segmentation, CAM, or modern classifiers)
>    - To allow flexible input sizes
>    - To reduce parameters and avoid flattening operations

6.Pooling layers help us to down-sample feature maps. How can we up-sample feature maps?

Loss Function

1.Why use weight decay in loss functions?

to prevent the weight too large,avoiding model complex

> [!NOTE]
>
> Weight decay, also known as **L2 regularization**, is used to **penalize large weights** in the model. It adds a term to the loss function proportional to the sum of the squared weights. This encourages the model to keep weights small, which:
>
> 1. Prevents overfitting
> 2. Simplifies the model
> 3. Improves generalization to unseen data

2.What is regularization? Why do we need it?
to prevent model overfitting,regularize the model

> [!NOTE]
>
> Regularization is a technique used in machine learning to **prevent overfitting** by adding additional constraints or penalties to the loss function. It discourages the model from fitting noise or overly complex patterns in the training data.
>
> Common regularization techniques include:
>
> - **L1/L2 regularization** (weight decay)
> - **Dropout**
> - **Early stopping**
> - **Data augmentation**
>
> We need regularization to ensure that the model generalizes well to new, unseen data.

1.Why do we need data pre-processing (e.g., data normalization)?

like the sigmoid ,the input is round zero,which is effective,otherwise,the resultt maybe occur gradiant disppear or esplosion 

> [!NOTE]
>
> Data preprocessing, especially **data normalization**, is essential to ensure that the input features are on a similar scale (typically with zero mean and unit variance). This is important for the following reasons:
>
> 1. **Stable and efficient training**â­
>     Inputs that are normalized allow the model to converge faster and more stably during training.
> 2. **Prevent vanishing/exploding gradients**
>     Especially in deep networks or with activation functions like sigmoid or tanh, unnormalized inputs can cause gradients to vanish or explode, making learning very difficult.
> 3. **Ensure fair contribution of each feature**â­
>     Normalization ensures that no single input feature dominates the learning just because of its scale.

2.What is over-fitting? How to reduce over-fitting?

è®­ç»ƒé›†ä¸Šæ•°æ®è®­ç»ƒæ•ˆæœå¥½ï¼Œæµ‹è¯•é›†ä¸Šæ•ˆæœä¸å¥½ï¼Œè¯´æ˜æ¨¡å‹åªæ˜¯åœ¨è®°å¿†ï¼Œè€Œä¸æ˜¯å­¦ä¹ ï¼Œä¸å…·æœ‰å¾ˆå¥½çš„æ³›åŒ–èƒ½åŠ›

é€šè¿‡æ­£åˆ™åŒ–æ¥å‡å°è¿‡æ‹Ÿåˆï¼ˆdata argumentã€L1ã€L2ã€Dropoutï¼‰

3.What is the difference between local optimum and global optimum? How to reduce local optimum?

- A **local optimum** is a point in the loss function where the function value is lower than in its nearby region, but **not necessarily the lowest** overall.
- A **global optimum** is the **absolute lowest** (for minimization problems) or highest (for maximization problems) point in the entire function domain.

In deep learning, due to the complex and non-convex loss landscapes, it's common for optimization to get stuck in local minima or saddle points.

Lossåœ¨æŸä¸ªåŒºåŸŸå†…æœ€å°ï¼Œè€Œä¸æ˜¯å…¨å±€æœ€å°

global optimum Losså…¨å±€æœ€å°

| Method                            | Description                                                  |
| --------------------------------- | ------------------------------------------------------------ |
| **Better initialization**         | Use advanced techniques like Xavier or He initialization     |
| **Use advanced optimizers**       | Optimizers like Adam, RMSProp can escape local traps better than vanilla SGD |
| **Add randomness**                | Random restarts or adding noise to gradients can help escape local optima |
| **Batch Normalization**           | Helps smooth the loss landscape                              |
| **Use larger models / more data** | Surprisingly, larger models are often more likely to find global optima or wide local optima |

| æ–¹æ³•                      | è¯´æ˜                                                         |
| ------------------------- | ------------------------------------------------------------ |
| **æƒé‡åˆå§‹åŒ–æŠ€å·§**        | ä½¿ç”¨ Xavier æˆ– He åˆå§‹åŒ–å¯ä»¥é¿å…ä¸€å¼€å§‹å°±æ‰è¿›åçš„å±€éƒ¨åŒºåŸŸ     |
| **æ”¹è¿›ä¼˜åŒ–å™¨**            | å¦‚ Adamã€RMSPropï¼Œç›¸è¾ƒæ™®é€š SGD æ›´å®¹æ˜“è·³å‡ºå±€éƒ¨æœ€ä¼˜            |
| **å¼•å…¥éšæœºæ€§**            | å¦‚éšæœºé‡å¯ï¼ˆrandom restartï¼‰æˆ–å‘æ¢¯åº¦ä¸­åŠ å…¥å™ªå£°ï¼ˆgradient noiseï¼‰ |
| **æ‰¹å½’ä¸€åŒ–ï¼ˆBatchNormï¼‰** | å¯å¹³æ»‘æŸå¤±æ›²é¢ï¼Œæœ‰åŠ©äºæ›´ç¨³å®šåœ°ä¸‹é™                           |
| **æ›´å¤§çš„æ¨¡å‹æˆ–æ›´å¤šæ•°æ®**  | åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå¤æ‚æ¨¡å‹åè€Œæ›´å¯èƒ½æ‰¾åˆ°æ›´å¥½çš„å…¨å±€æˆ–å®½é˜”çš„å±€éƒ¨æœ€ä¼˜è§£ |

4.What are vanishing gradient and exploding gradient? How to reduce vanishing gradient and exploding gradient?

æ¢¯åº¦æ¶ˆå¤±å°±æ˜¯æ¢¯åº¦å˜ä¸ºå¾ˆå°å¾ˆå°ï¼Œæ¢¯åº¦çˆ†ç‚¸å°±æ˜¯æ¢¯åº¦å˜æˆæ— ç©·å¤§

ä½¿ç”¨relu(RELUä½œä¸ºä¸€ç§ä¸é¥±å’Œç¥ç»å…ƒï¼Œä»–åªè¦æ­£è¾“å…¥å°±ä¼šå­¦ä¹ ï¼ˆä¸éœ€è¦å½’ä¸€åŒ–å¤„ç†,ä½†æ˜¯è®ºæ–‡ä¸­ç”¨äº†LRNï¼‰ï¼Œè€Œsigmoidæˆ–è€…tanhåªè¦è¾“å…¥è¿‡å¤§æˆ–è¿‡å°ï¼Œæ¢¯åº¦ä¼šæ¶ˆå¤±ï¼Œåœæ­¢å­¦ä¹ ï¼Œåƒè¿™ç§é¥±å’Œæ¿€æ´»å‡½æ•°ä¸€èˆ¬éœ€è¦å¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–å¤„ç†)ï¼ŒBN,resnet connectionã€‚ã€‚ã€‚ï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰

| Problem                 | Solution                                                     |
| :---------------------- | :----------------------------------------------------------- |
| **Vanishing gradients** | - Use **ReLU** instead of sigmoid/tanh                                                                                           - Apply **Batch Normalization **                                                                                                           - Use **residual connections** (e.g., in ResNet)                                                                                     - Initialize weights properly (e.g., Xavier/He initialization) |
| **Exploding gradients** | - **Gradient clipping **                                                                                                                         - Proper **weight initialization **                                                                                                               - Use **normalization techniques** like BatchNorm |

5.How will training hyper-parameters (e.g., learning rate, batch size and the number of iterations) affect the network training?

 learning rate too large may miss the global optimum,too small,cost too much training time

batch size 

the number of iterations too large may overfit

|               è¶…å‚æ•°                |                           å½±å“è¯´æ˜                           |
| :---------------------------------: | :----------------------------------------------------------: |
|     **å­¦ä¹ ç‡ï¼ˆLearning rateï¼‰**     | å¦‚æœå­¦ä¹ ç‡å¤ªå¤§ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹è·³è¿‡æœ€ä¼˜è§£ç”šè‡³å‘æ•£ï¼›å¦‚æœå¤ªå°ï¼Œè®­ç»ƒæ”¶æ•›å¤ªæ…¢ï¼Œå¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚ |
|      **æ‰¹å¤§å°ï¼ˆBatch sizeï¼‰**       | å°æ‰¹é‡è®­ç»ƒå¸¦æ¥æ¢¯åº¦çš„éšæœºæ€§ï¼Œæœ‰åŠ©äºè·³å‡ºå±€éƒ¨æœ€ä¼˜ï¼Œæå‡æ³›åŒ–èƒ½åŠ›ï¼Œä½†è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼›å¤§æ‰¹é‡è®­ç»ƒæ›´ç¨³å®šï¼Œè®­ç»ƒé€Ÿåº¦å¿«ï¼Œä½†å¯èƒ½é™·å…¥ä¸å¥½çš„å±€éƒ¨æœ€ä¼˜ï¼Œå¹¶éœ€è¦æ›´å¤šå†…å­˜ã€‚ |
| **è¿­ä»£æ¬¡æ•°ï¼ˆIterations / Epochsï¼‰** | è¿­ä»£æ¬¡æ•°å¤ªå°‘æ¨¡å‹æ¬ æ‹Ÿåˆï¼Œå­¦ä¹ ä¸å……åˆ†ï¼›å¤ªå¤šå¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œå­¦ä¹ äº†è®­ç»ƒæ•°æ®ä¸­çš„å™ªå£°ï¼Œæ³›åŒ–èƒ½åŠ›å·®ã€‚ç†æƒ³çš„è¿­ä»£æ¬¡æ•°éœ€è¦é€šè¿‡éªŒè¯é›†æ¥ç¡®å®šã€‚ |

1.Why do residual connections enable deeper networks?

residual connections disolve the difficulty of degrading, é€šè¿‡è®­ç»ƒæ®‹å·®å‡½æ•°ï¼Œshortcut(extremely,identity),

> [!NOTE]
>
> Residual connections solve the **degradation problem**, where adding more layers actually **worsens** the training accuracy.
>
> In a residual block, instead of learning the target output `H(x)`, the network learns the **residual** `F(x) = H(x) - x`. The final output becomes `y = F(x) + x`. This "shortcut connection" (often just an identity mapping) allows gradients to **flow more easily** through the network during backpropagation, avoiding vanishing gradients and helping the network **train much deeper architectures**.
>
> > In short: residual connections make it easier to optimize deep networks by preserving the information across layers and simplifying what each block needs to learn.



Will the resolution of the input image affect the network performance?

æ˜¯çš„ï¼Œè¾“å…¥å›¾åƒçš„åˆ†è¾¨ç‡**ä¼šå¯¹ç¥ç»ç½‘ç»œçš„æ€§èƒ½äº§ç”Ÿæ˜¾è‘—å½±å“**ï¼Œè¿™å…¶ä¸­æ¶‰åŠä¸€ç³»åˆ—æƒè¡¡å’Œè€ƒé‡ã€‚æ²¡æœ‰ä¸€ä¸ªâ€œæœ€ä½³â€çš„åˆ†è¾¨ç‡é€‚ç”¨äºæ‰€æœ‰æƒ…å†µï¼Œé€‰æ‹©åˆé€‚çš„è¾“å…¥åˆ†è¾¨ç‡å–å†³äºå…·ä½“çš„ä»»åŠ¡ã€å¯ç”¨çš„è®¡ç®—èµ„æºä»¥åŠæœŸæœ›çš„æ€§èƒ½ã€‚

ä»¥ä¸‹æ˜¯è¾“å…¥å›¾åƒåˆ†è¾¨ç‡å½±å“ç¥ç»ç½‘ç»œæ€§èƒ½çš„å‡ ä¸ªä¸»è¦æ–¹é¢ï¼š

1. ä¿¡æ¯é‡å’Œç»†èŠ‚ä¿ç•™

- **é«˜åˆ†è¾¨ç‡å›¾åƒï¼š** åŒ…å«æ›´å¤šçš„åƒç´ å’Œæ›´ä¸°å¯Œçš„ç»†èŠ‚ä¿¡æ¯ã€‚å¯¹äºéœ€è¦æ•æ‰ç»†å¾®ç‰¹å¾çš„ä»»åŠ¡ï¼Œå¦‚åŒ»å­¦å›¾åƒè¯Šæ–­ï¼ˆæ£€æµ‹å¾®å°ç—…å˜ï¼‰ã€ç›®æ ‡æ£€æµ‹ï¼ˆè¯†åˆ«å°ç‰©ä½“ï¼‰ã€è¯­ä¹‰åˆ†å‰²ï¼ˆç²¾ç¡®åˆ’åˆ†ç‰©ä½“è¾¹ç•Œï¼‰ç­‰ï¼Œé«˜åˆ†è¾¨ç‡å›¾åƒå¯ä»¥æä¾›æ¨¡å‹åšå‡ºå‡†ç¡®åˆ¤æ–­æ‰€éœ€çš„æ›´å¤šè§†è§‰è¯æ®ï¼Œä»è€Œæé«˜æ¨¡å‹ç²¾åº¦ã€‚
- **ä½åˆ†è¾¨ç‡å›¾åƒï¼š** ä¼šä¸¢å¤±å›¾åƒä¸­çš„ç»†ç²’åº¦ç»†èŠ‚å’Œé«˜é¢‘ä¿¡æ¯ã€‚å¯¹äºä¸€äº›å¯¹ç»†èŠ‚è¦æ±‚ä¸é«˜çš„ä»»åŠ¡ï¼Œä¾‹å¦‚ç²—ç²’åº¦å›¾åƒåˆ†ç±»ï¼ˆåŒºåˆ†çŒ«ç‹—ï¼‰ï¼Œä½åˆ†è¾¨ç‡å›¾åƒå¯èƒ½å°±è¶³å¤Ÿäº†ã€‚ä½†å¦‚æœä»»åŠ¡éœ€è¦è¯†åˆ«å¾®å°çš„å·®å¼‚æˆ–çº¹ç†ï¼Œä½åˆ†è¾¨ç‡ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

2. è®¡ç®—èµ„æºå’Œè®­ç»ƒæ—¶é—´

- è®¡ç®—å¼€é”€å¢åŠ ï¼š

   éšç€å›¾åƒåˆ†è¾¨ç‡çš„æé«˜ï¼Œæ¯ä¸ªåƒç´ ç‚¹éƒ½éœ€è¦è¢«å¤„ç†ã€‚è¿™ä¼šå¯¼è‡´ï¼š

  - **å†…å­˜å ç”¨å¢åŠ ï¼š** é«˜åˆ†è¾¨ç‡å›¾åƒéœ€è¦æ›´å¤šçš„æ˜¾å­˜æ¥å­˜å‚¨ã€‚
  - **è®¡ç®—é‡å¢åŠ ï¼š** å·ç§¯æ“ä½œã€æ± åŒ–æ“ä½œç­‰è¿ç®—é‡ä¼šéšè¾“å…¥å°ºå¯¸çš„å¹³æ–¹å¢åŠ ï¼Œå¯¼è‡´è®­ç»ƒå’Œæ¨ç†æ—¶é—´æ˜¾è‘—å¢åŠ ã€‚
  - **å‚æ•°æ•°é‡å¢åŠ ï¼ˆå¯¹äºå…¨è¿æ¥å±‚ï¼‰ï¼š** å¦‚æœç½‘ç»œä¸­åŒ…å«å…¨è¿æ¥å±‚ï¼Œè¾“å…¥åˆ†è¾¨ç‡çš„å¢åŠ ä¼šç›´æ¥å¯¼è‡´å…¨è¿æ¥å±‚å‚æ•°æ•°é‡çš„æ€¥å‰§å¢åŠ ï¼Œè¿›ä¸€æ­¥åŠ é‡è®¡ç®—è´Ÿæ‹…ã€‚

- **è®­ç»ƒé€Ÿåº¦å˜æ…¢ï¼š** æ›´å¤§çš„è®¡ç®—é‡æ„å‘³ç€æ›´é•¿çš„è®­ç»ƒæ—¶é—´ã€‚è¿™ä¼šå»¶é•¿æ¨¡å‹å¼€å‘å‘¨æœŸï¼Œå¹¶å¯èƒ½é™åˆ¶æ‚¨è¿›è¡Œå®éªŒå’Œè¶…å‚æ•°è°ƒä¼˜çš„èƒ½åŠ›ã€‚

3. æ¨¡å‹æ³›åŒ–èƒ½åŠ›å’Œè¿‡æ‹Ÿåˆ

- **ä¿¡æ¯å†—ä½™å’Œå™ªéŸ³ï¼š** è¿‡é«˜çš„åˆ†è¾¨ç‡å¯èƒ½ä¼šå¼•å…¥ä¸€äº›å¯¹ä»»åŠ¡ä¸å¿…è¦çš„å†—ä½™ä¿¡æ¯æˆ–å™ªå£°ï¼Œå¦‚æœæ¨¡å‹å®¹é‡ä¸è¶³æˆ–æ­£åˆ™åŒ–ä¸å½“ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®é›†è§„æ¨¡ä¸å¤Ÿå¤§çš„æƒ…å†µä¸‹ã€‚
- å…³æ³¨å…¨å±€ vs. å±€éƒ¨ç‰¹å¾ï¼š
  - ä½åˆ†è¾¨ç‡å›¾åƒå¯èƒ½ä¿ƒä½¿ç½‘ç»œæ›´å…³æ³¨å›¾åƒçš„å…¨å±€ç»“æ„å’Œä½çº§ç‰¹å¾ã€‚
  - é«˜åˆ†è¾¨ç‡å›¾åƒåˆ™å…è®¸ç½‘ç»œå­¦ä¹ æ›´ç²¾ç»†çš„å±€éƒ¨ç‰¹å¾ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¿‡å¤šçš„å±€éƒ¨ç»†èŠ‚å¯èƒ½åˆ†æ•£ç½‘ç»œå¯¹å…¨å±€æ¨¡å¼çš„æ³¨æ„åŠ›ã€‚
- **æ„Ÿå—é‡ï¼š** ç¥ç»ç½‘ç»œä¸­æ¯ä¸ªç¥ç»å…ƒçš„æ„Ÿå—é‡æ˜¯æœ‰é™çš„ã€‚å¯¹äºé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œå¦‚æœç½‘ç»œæ·±åº¦ä¸å¤Ÿæˆ–å·ç§¯æ ¸å°ºå¯¸å¤ªå°ï¼Œå¯èƒ½æ— æ³•æœ‰æ•ˆæ•æ‰å›¾åƒä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»æˆ–å¤§å°ºåº¦ç‰©ä½“ã€‚

4. è¿ç§»å­¦ä¹ å’Œé¢„è®­ç»ƒæ¨¡å‹

- é¢„è®­ç»ƒæ¨¡å‹é™åˆ¶ï¼š

   å¤§å¤šæ•°å¸¸ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ ImageNet ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼‰éƒ½æœ‰å›ºå®šçš„è¾“å…¥åˆ†è¾¨ç‡ï¼ˆä¾‹å¦‚ 224x224, 299x299ï¼‰ã€‚å¦‚æœæ‚¨ä½¿ç”¨ä¸åŒåˆ†è¾¨ç‡çš„å›¾åƒï¼Œéœ€è¦å¯¹å›¾åƒè¿›è¡Œç¼©æ”¾ã€‚

  - **ä¸‹é‡‡æ ·ï¼š** å°†é«˜åˆ†è¾¨ç‡å›¾åƒç¼©æ”¾åˆ°é¢„è®­ç»ƒæ¨¡å‹æ‰€éœ€çš„å°ºå¯¸ä¼šå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ã€‚
  - **ä¸Šé‡‡æ ·ï¼š** å°†ä½åˆ†è¾¨ç‡å›¾åƒæ”¾å¤§åˆ°æ‰€éœ€å°ºå¯¸ä¼šå¼•å…¥æ’å€¼ä¼ªå½±ï¼Œå¹¶ä¸èƒ½çœŸæ­£å¢åŠ ä¿¡æ¯é‡ã€‚

- **å¾®è°ƒï¼š** å¦‚æœä½¿ç”¨ä¸åŒåˆ†è¾¨ç‡çš„è¾“å…¥å›¾åƒï¼Œå¯èƒ½éœ€è¦å¯¹æ•´ä¸ªç½‘ç»œè¿›è¡Œå¾®è°ƒï¼Œè€Œä¸ä»…ä»…æ˜¯æœ€åä¸€å±‚ï¼Œå› ä¸ºæ—©æœŸçš„å·ç§¯å±‚ä¹Ÿéœ€è¦é€‚åº”æ–°çš„è¾“å…¥å°ºåº¦æ¥æå–æœ‰æ•ˆçš„ç‰¹å¾ã€‚

5. ä»»åŠ¡ç‰¹å®šè¦æ±‚

- **åˆ†ç±»ä»»åŠ¡ï¼š** å¯¹äºä¸€äº›ç®€å•çš„åˆ†ç±»ä»»åŠ¡ï¼Œå¯èƒ½ä¸éœ€è¦éå¸¸é«˜çš„åˆ†è¾¨ç‡ã€‚ä¾‹å¦‚ï¼ŒåŒºåˆ†çŒ«å’Œç‹—ï¼Œ224x224 çš„å›¾åƒé€šå¸¸å°±è¶³å¤Ÿäº†ã€‚
- **ç›®æ ‡æ£€æµ‹å’Œåˆ†å‰²ï¼š** è¿™äº›ä»»åŠ¡é€šå¸¸å—ç›Šäºæ›´é«˜çš„åˆ†è¾¨ç‡ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦ç²¾ç¡®çš„å®šä½å’Œè¾¹ç•Œè¯†åˆ«ã€‚å°ç›®æ ‡åœ¨ä½åˆ†è¾¨ç‡å›¾åƒä¸­å¯èƒ½å®Œå…¨æ¶ˆå¤±ã€‚
- **è¶…åˆ†è¾¨ç‡ï¼š** æœ¬èº«å°±æ˜¯å°†ä½åˆ†è¾¨ç‡å›¾åƒè½¬æ¢ä¸ºé«˜åˆ†è¾¨ç‡å›¾åƒçš„ä»»åŠ¡ï¼Œå…¶æ€§èƒ½ç›´æ¥ä¾èµ–äºå¯¹ä½åˆ†è¾¨ç‡è¾“å…¥ä¸­ä¿¡æ¯çš„æœ‰æ•ˆæå–å’Œé«˜åˆ†è¾¨ç‡ç»†èŠ‚çš„é‡å»ºèƒ½åŠ›ã€‚

### æ€»ç»“

è¾“å…¥å›¾åƒåˆ†è¾¨ç‡çš„é€‰æ‹©æ˜¯ä¸€ä¸ª**æƒè¡¡è¿‡ç¨‹**ã€‚æ‚¨éœ€è¦åœ¨**æ¨¡å‹æ€§èƒ½ï¼ˆç²¾åº¦ã€ç»†èŠ‚æ•æ‰èƒ½åŠ›ï¼‰**å’Œ**è®¡ç®—æ•ˆç‡ï¼ˆå†…å­˜ã€è®­ç»ƒæ—¶é—´ï¼‰**ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ç‚¹ã€‚

- **å¯¹äºéœ€è¦ç²¾ç»†ç»†èŠ‚çš„ä»»åŠ¡ï¼Œå°½å¯èƒ½ä½¿ç”¨é«˜åˆ†è¾¨ç‡ã€‚** ä½†åŒæ—¶è¦è€ƒè™‘è®¡ç®—èµ„æºæ˜¯å¦æ”¯æŒï¼Œå¹¶å¯èƒ½éœ€è¦æ›´æ·±ã€æ›´å¤æ‚çš„ç½‘ç»œç»“æ„æ¥æœ‰æ•ˆå¤„ç†è¿™äº›ä¿¡æ¯ã€‚
- **å¯¹äºè®¡ç®—èµ„æºæœ‰é™æˆ–å¯¹ç»†èŠ‚è¦æ±‚ä¸é«˜çš„ä»»åŠ¡ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨è¾ƒä½åˆ†è¾¨ç‡ã€‚**
- **å¤šå°ºåº¦è®­ç»ƒå’ŒåŠ¨æ€åˆ†è¾¨ç‡ç½‘ç»œ** æ˜¯åº”å¯¹ä¸åŒåˆ†è¾¨ç‡æŒ‘æˆ˜çš„å…ˆè¿›æ–¹æ³•ï¼Œå®ƒä»¬å…è®¸ç½‘ç»œåœ¨è®­ç»ƒæˆ–æ¨ç†æ—¶å¤„ç†å¤šç§åˆ†è¾¨ç‡çš„è¾“å…¥ï¼Œä»¥å…¼é¡¾æ€§èƒ½å’Œæ•ˆç‡ã€‚

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé€šå¸¸ä¼šé€šè¿‡å®éªŒæ¥ç¡®å®šæœ€ä½³çš„è¾“å…¥åˆ†è¾¨ç‡ã€‚



Normally, what kinds of object instances are hard to be detected?

ä»¥ä¸‹æ˜¯ä¸€äº›é€šå¸¸éš¾ä»¥æ£€æµ‹çš„ç›®æ ‡å®ä¾‹ç±»å‹ï¼š

1. **å°ç›®æ ‡ (Small Objects)**:
   - **åƒç´ ä¿¡æ¯å°‘ï¼š** å°ç›®æ ‡åœ¨å›¾åƒä¸­åªå æ®å¾ˆå°‘çš„åƒç´ ï¼Œè¿™æ„å‘³ç€å®ƒä»¬åŒ…å«çš„è§†è§‰ä¿¡æ¯éå¸¸æœ‰é™ã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ç»è¿‡å¤šå±‚å·ç§¯å’Œæ± åŒ–åï¼Œè¿™äº›å¾®å¼±çš„ç‰¹å¾å¾ˆå®¹æ˜“åœ¨ç‰¹å¾å›¾ä¸­ä¸¢å¤±ã€‚
   - **ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ï¼š** ç”±äºå°ºå¯¸å°ï¼Œå‘¨å›´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ä¹Ÿå¯èƒ½ä¸æ˜æ˜¾æˆ–éš¾ä»¥åˆ©ç”¨ã€‚
   - **ä½åˆ†è¾¨ç‡ï¼š** å°ç›®æ ‡é€šå¸¸ä¸ä½åˆ†è¾¨ç‡å›¾åƒç›¸å…³è”ï¼Œè¿›ä¸€æ­¥åŠ å‰§äº†ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ã€‚
   - **åº”ç”¨åœºæ™¯ï¼š** è‡ªåŠ¨é©¾é©¶ä¸­çš„è¿œè·ç¦»è¡Œäººæˆ–è½¦è¾†ã€å«æ˜Ÿå›¾åƒä¸­çš„å°å‹å»ºç­‘ç‰©ã€åŒ»å­¦å›¾åƒä¸­çš„å¾®å°ç—…å˜ç­‰ã€‚
2. **é®æŒ¡ç›®æ ‡ (Occluded Objects)**:
   - **éƒ¨åˆ†å¯è§ï¼š** å½“ä¸€ä¸ªç›®æ ‡è¢«å¦ä¸€ä¸ªç‰©ä½“ï¼ˆæˆ–å…¶ä»–ç›¸åŒç›®æ ‡ï¼‰éƒ¨åˆ†é®æŒ¡æ—¶ï¼Œæ¨¡å‹çš„è¾“å…¥ä¸­åªæœ‰éƒ¨åˆ†ç‰¹å¾å¯ç”¨ã€‚äººç±»å¯ä»¥å¾ˆå®¹æ˜“åœ°é€šè¿‡ä¸Šä¸‹æ–‡å’Œå¯¹ç‰©ä½“å½¢çŠ¶çš„å…ˆéªŒçŸ¥è¯†æ¥è¯†åˆ«è¢«é®æŒ¡çš„ç‰©ä½“ï¼Œä½†å¯¹äºæ¨¡å‹æ¥è¯´ï¼Œè¿™è¦å›°éš¾å¾—å¤šã€‚
   - **ç‰¹å¾ç¼ºå¤±ï¼š** å…³é”®çš„è¾¨åˆ«ç‰¹å¾å¯èƒ½è¢«é®æŒ¡ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥å‡†ç¡®è¯†åˆ«å’Œå®šä½ã€‚
   - **è¾¹ç•Œæ¨¡ç³Šï¼š** é®æŒ¡ç‰©å’Œè¢«é®æŒ¡ç‰©ä¹‹é—´çš„è¾¹ç•Œå¯èƒ½å˜å¾—æ¨¡ç³Šï¼Œä½¿å¾—å‡†ç¡®ç»˜åˆ¶è¾¹ç•Œæ¡†å˜å¾—å›°éš¾ã€‚
   - **åº”ç”¨åœºæ™¯ï¼š** æ‹¥æŒ¤è¡—é“ä¸Šçš„è¡Œäººã€è¢«è½¦è¾†é®æŒ¡çš„äº¤é€šæ ‡å¿—ã€å †å çš„å•†å“ç­‰ã€‚
3. **å¯†é›†ç›®æ ‡ (Crowded/Dense Objects)**:
   - **é«˜é‡å åº¦ï¼š** åœ¨æ‹¥æŒ¤çš„åœºæ™¯ä¸­ï¼Œå¤šä¸ªç›®æ ‡å®ä¾‹å¯èƒ½ä¼šé«˜åº¦é‡å ï¼Œå¯¼è‡´å®ƒä»¬çš„è¾¹ç•Œæ¡†éš¾ä»¥åŒºåˆ†ã€‚
   - **éæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰çš„æŒ‘æˆ˜ï¼š** ä¼ ç»Ÿçš„ç›®æ ‡æ£€æµ‹æµç¨‹ä¸­çš„ NMS ç®—æ³•åœ¨å¤„ç†é«˜åº¦é‡å çš„ç›®æ ‡æ—¶å¯èƒ½ä¼šå°†å¤šä¸ªçœŸå®ç›®æ ‡æŠ‘åˆ¶æ‰ï¼Œå¯¼è‡´æ¼æ£€ã€‚
   - **ä¸Šä¸‹æ–‡æ··æ·†ï¼š** ç›®æ ‡ä¹‹é—´ç›¸äº’å½±å“ï¼Œå¢åŠ äº†æ¨¡å‹ç†è§£æ¯ä¸ªç‹¬ç«‹ç›®æ ‡éš¾åº¦ã€‚
   - **åº”ç”¨åœºæ™¯ï¼š** ä½“è‚²åœºè§‚ä¼—ã€äººæµå¯†é›†åŒºåŸŸã€é¸Ÿç¾¤ã€å¯†é›†æ’åˆ—çš„å•†å“ç­‰ã€‚
4. **å½¢å˜ç›®æ ‡ (Deformable Objects)**:
   - **å½¢çŠ¶ä¸å›ºå®šï¼š** è®¸å¤šç›®æ ‡ä¸æ˜¯åˆšä½“ï¼Œå¯ä»¥ä»¥å¤šç§æ–¹å¼å˜å½¢ï¼ˆä¾‹å¦‚ï¼Œäººä½“æ‘†å‡ºä¸åŒå§¿åŠ¿ã€ç»³ç´¢ã€è›‡ã€æœè£…ï¼‰ã€‚è¿™æ„å‘³ç€åŒä¸€ä¸ªç‰©ä½“åœ¨ä¸åŒå§¿æ€ä¸‹ä¼šæœ‰éå¸¸ä¸åŒçš„è§†è§‰è¡¨ç°ã€‚
   - **ç‰¹å¾æå–å›°éš¾ï¼š** ä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œå¯èƒ½éš¾ä»¥æ•æ‰è¿™ç§é«˜åº¦å˜åŒ–çš„å‡ ä½•ç‰¹å¾ã€‚
   - **åº”ç”¨åœºæ™¯ï¼š** ç‘œä¼½å§¿åŠ¿çš„äººã€è·³èˆçš„äººã€ä¸åŒå½¢çŠ¶çš„å¸ƒæ–™ã€è½¯ä½“æœºå™¨äººç­‰ã€‚
5. **ç±»å†…å·®å¼‚å¤§ç›®æ ‡ (Objects with Large Intra-Class Variation)**:
   - **å¤–è§‚å¤šæ ·æ€§ï¼š** å³ä½¿æ˜¯åŒä¸€ç±»åˆ«çš„ç›®æ ‡ï¼Œå…¶å¤–è§‚ä¹Ÿå¯èƒ½åƒå·®ä¸‡åˆ«ï¼ˆä¾‹å¦‚ï¼Œä¸åŒå“ç‰Œã€å‹å·ã€é¢œè‰²çš„æ±½è½¦ï¼›å„ç§å„æ ·çš„æ¤…å­ï¼‰ã€‚
   - **ç‰¹å¾æ³›åŒ–éš¾ï¼š** æ¨¡å‹éœ€è¦å­¦ä¹ èƒ½å¤Ÿæ³›åŒ–åˆ°æ‰€æœ‰è¿™äº›å˜ä½“çš„ç‰¹å¾ï¼Œè€Œä¸æ˜¯è¿‡åº¦æ‹Ÿåˆç‰¹å®šæ ·æœ¬ã€‚
   - **åº”ç”¨åœºæ™¯ï¼š** å„ç§å®¶å…·ã€ä¸åŒå“ç§çš„ç‹—ã€ä¸åŒç±»å‹çš„æœè£…ã€è‰ºæœ¯å“ç­‰ã€‚
6. **èƒŒæ™¯å¤æ‚æˆ–ä¼ªè£…ç›®æ ‡ (Objects in Cluttered or Camouflaged Backgrounds)**:
   - **ä¸èƒŒæ™¯èåˆï¼š** å½“ç›®æ ‡ä¸èƒŒæ™¯çš„é¢œè‰²ã€çº¹ç†æˆ–äº®åº¦éå¸¸ç›¸ä¼¼æ—¶ï¼Œç›®æ ‡ä¼šâ€œèå…¥â€èƒŒæ™¯ï¼Œä½¿å…¶éš¾ä»¥è¢«åŒºåˆ†ã€‚
   - **ç‰¹å¾åŒºåˆ†åº¦ä½ï¼š** æ¨¡å‹çš„ç‰¹å¾æå–å™¨å¯èƒ½æ— æ³•æœ‰æ•ˆåœ°åŒºåˆ†ç›®æ ‡å’ŒèƒŒæ™¯åƒç´ ã€‚
   - **åº”ç”¨åœºæ™¯ï¼š** å†›äº‹ä¼ªè£…ã€åŠ¨ç‰©ä¿æŠ¤è‰²ã€æ‚ä¹±åŠå…¬æ¡Œä¸Šçš„å°ç‰©ä»¶ç­‰ã€‚
7. **å…‰ç…§æˆ–è§†è§’æç«¯ç›®æ ‡ (Objects under Extreme Illumination or Viewpoint Variations)**:
   - **å…‰ç…§ï¼š** æäº®ã€ææš—ã€å¼ºé˜´å½±ã€é€†å…‰ç­‰å…‰ç…§æ¡ä»¶ä¼šå¤§å¹…æ”¹å˜ç›®æ ‡çš„åƒç´ å€¼ï¼Œä½¿å…¶çœ‹èµ·æ¥ä¸è®­ç»ƒæ•°æ®ä¸­çš„æ­£å¸¸å…‰ç…§ä¸‹çš„æ ·æœ¬å·®å¼‚å¾ˆå¤§ã€‚
   - **è§†è§’ï¼š** åŒä¸€ä¸ªç›®æ ‡ä»ä¸åŒè§’åº¦çœ‹å¯èƒ½å®Œå…¨ä¸åŒã€‚å¦‚æœè®­ç»ƒæ•°æ®æ²¡æœ‰å……åˆ†è¦†ç›–æ‰€æœ‰å¯èƒ½çš„è§†è§’ï¼Œæ¨¡å‹å¯èƒ½éš¾ä»¥è¯†åˆ«ã€‚
   - **åº”ç”¨åœºæ™¯ï¼š** å¤œé—´ç›‘æ§ã€å¼ºå…‰ä¸‹çš„å®¤å¤–åœºæ™¯ã€èˆªæ‹å›¾åƒä¸­çš„å€¾æ–œå»ºç­‘ç‰©ç­‰ã€‚
8. **ç½•è§ç›®æ ‡æˆ–é•¿å°¾åˆ†å¸ƒç›®æ ‡ (Rare or Long-Tail Objects)**:
   - **æ•°æ®ç¨€ç¼ºï¼š** åœ¨æ•°æ®é›†ä¸­å‡ºç°é¢‘ç‡å¾ˆä½çš„ç›®æ ‡å®ä¾‹ï¼Œå¯¼è‡´æ¨¡å‹åœ¨è®­ç»ƒæ—¶æ¥è§¦åˆ°çš„æ ·æœ¬å¾ˆå°‘ï¼Œå­¦ä¹ ä¸è¶³ã€‚
   - **ç±»ä¸å¹³è¡¡é—®é¢˜ï¼š** ç½•è§ç±»åˆ«åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®¹æ˜“è¢«å¤§ç±»åˆ«æ·¹æ²¡ï¼Œå¯¼è‡´æ¨¡å‹å¯¹å®ƒä»¬ä¸å¤Ÿæ•æ„Ÿã€‚
   - **åº”ç”¨åœºæ™¯ï¼š** ç‰¹å®šé‡ç”ŸåŠ¨ç‰©ç‰©ç§ã€ç½•è§ç–¾ç—…çš„ç—…ç¶ã€æŸäº›å·¥ä¸šæ•…éšœç±»å‹ç­‰ã€‚

ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆä»¬åœ¨ç›®æ ‡æ£€æµ‹é¢†åŸŸä¸æ–­å‘å±•å‡ºå„ç§å…ˆè¿›çš„æŠ€æœ¯ï¼Œä¾‹å¦‚ï¼š

- å¤šå°ºåº¦ç‰¹å¾èåˆç½‘ç»œ (FPNs)

   ç”¨äºå¤„ç†å°ç›®æ ‡ã€‚

- æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanisms)

   å¸®åŠ©æ¨¡å‹å…³æ³¨é‡è¦åŒºåŸŸã€‚

- **æ›´å¼ºå¤§çš„éª¨å¹²ç½‘ç»œ (Backbone Networks)** æå–æ›´é²æ£’çš„ç‰¹å¾ã€‚

- **æ•°æ®å¢å¼º (Data Augmentation)** æŠ€æœ¯ï¼ˆå¦‚éšæœºè£å‰ªã€é®æŒ¡æ¨¡æ‹Ÿï¼‰æ¥æ¨¡æ‹Ÿå¤æ‚åœºæ™¯ã€‚

- **æ›´å…ˆè¿›çš„ NMS å˜ä½“ (e.g., Soft-NMS, DIoU-NMS)** å¤„ç†å¯†é›†å’Œé‡å ç›®æ ‡ã€‚

- **åŸºäºTransformerçš„æ£€æµ‹å™¨ (e.g., DETR)** å‡å°‘å¯¹æ‰‹å·¥è®¾è®¡å…ˆéªŒçš„ä¾èµ–ï¼Œå¹¶æ›´å¥½åœ°å¤„ç†å…¨å±€ä¸Šä¸‹æ–‡ã€‚

- **åº¦é‡å­¦ä¹ å’Œé¢†åŸŸé€‚åº” (Metric Learning and Domain Adaptation)** æ¥å¤„ç†ç±»å†…å·®å¼‚å’Œæ•°æ®åˆ†å¸ƒä¸ä¸€è‡´çš„é—®é¢˜ã€‚

å°½ç®¡å¦‚æ­¤ï¼Œä¸Šè¿°æŒ‘æˆ˜ä»ç„¶æ˜¯å½“å‰ç›®æ ‡æ£€æµ‹ç ”ç©¶çš„çƒ­ç‚¹å’Œéš¾ç‚¹ã€‚



Please explain object detection metrics: precision, recall, F1-score, AP, AR, mAP, ROC curve and AUC. What are the functions of these metrics? How to calculate them?

### 1. æ··æ·†çŸ©é˜µåŸºç¡€ (Confusion Matrix Basics)

åœ¨ç†è§£è¿™äº›æŒ‡æ ‡ä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ç†è§£ç›®æ ‡æ£€æµ‹ä¸­çš„åŸºæœ¬åˆ†ç±»ç»“æœï¼š

- **çœŸé˜³æ€§ (True Positive, TP)ï¼š** æ¨¡å‹æ­£ç¡®åœ°æ£€æµ‹åˆ°å¹¶è¯†åˆ«å‡ºçš„ç›®æ ‡å®ä¾‹ã€‚å³ï¼Œæ£€æµ‹æ¡†ä¸çœŸå®ç›®æ ‡æ¡†çš„ IoU (Intersection over Union) å¤§äºæŸä¸ªé¢„è®¾é˜ˆå€¼ï¼ˆä¾‹å¦‚ 0.5ï¼‰ã€‚
- **å‡é˜³æ€§ (False Positive, FP)ï¼š** æ¨¡å‹å°†èƒŒæ™¯é”™è¯¯åœ°æ£€æµ‹ä¸ºä¸€ä¸ªç›®æ ‡ï¼Œæˆ–è€…æ£€æµ‹åˆ°äº†ä¸€ä¸ªç›®æ ‡ä½†å…¶æ£€æµ‹æ¡†ä¸çœŸå®ç›®æ ‡æ¡†çš„ IoU ä½äºé˜ˆå€¼ï¼Œæˆ–è€…å¯¹åŒä¸€ä¸ªçœŸå®ç›®æ ‡è¿›è¡Œäº†é‡å¤æ£€æµ‹ã€‚
- **å‡é˜´æ€§ (False Negative, FN)ï¼š** æ¨¡å‹æœªèƒ½æ£€æµ‹åˆ°å®é™…å­˜åœ¨çš„æŸä¸ªç›®æ ‡å®ä¾‹ï¼ˆæ¼æ£€ï¼‰ã€‚
- **çœŸé˜´æ€§ (True Negative, TN)ï¼š** ç›®æ ‡æ£€æµ‹é€šå¸¸ä¸å…³æ³¨çœŸé˜´æ€§ã€‚å› ä¸ºåœ¨å›¾åƒä¸­ç»å¤§éƒ¨åˆ†åŒºåŸŸéƒ½æ˜¯èƒŒæ™¯ï¼Œå¦‚æœè®¡ç®—çœŸé˜´æ€§ï¼Œä¼šå¯¼è‡´è¿™ä¸ªå€¼éå¸¸å¤§ä¸”æ²¡æœ‰å®é™…æ„ä¹‰ã€‚æ‰€ä»¥ï¼Œåœ¨ç›®æ ‡æ£€æµ‹ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¸è€ƒè™‘ TNã€‚

------

### 2. ç²¾ç¡®ç‡ (Precision)

- **å®šä¹‰ï¼š** Precision è¡¡é‡çš„æ˜¯æ¨¡å‹æ£€æµ‹å‡ºçš„ç›®æ ‡ä¸­ï¼Œæœ‰å¤šå°‘æ¯”ä¾‹æ˜¯**çœŸæ­£æ­£ç¡®çš„**ã€‚å®ƒå…³æ³¨çš„æ˜¯æ¨¡å‹**ä¸æŠ¥å‡è­¦**çš„èƒ½åŠ›ã€‚

  

  

- **ä½œç”¨ï¼š** å¯¹äºé‚£äº›è¦æ±‚è¯¯æŠ¥ç‡ä½çš„ä»»åŠ¡éå¸¸é‡è¦ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ä¸­çš„ç´§æ€¥åˆ¶åŠ¨ç³»ç»Ÿï¼ˆè¯¯æŠ¥å¯èƒ½å¯¼è‡´ä¸å¿…è¦çš„æ€¥åˆ¹è½¦ï¼‰ã€‚

- **è®¡ç®—å…¬å¼ï¼š**    Precision=TP+FPTP

  

  

  - **TP + FP**ï¼šè¡¨ç¤ºæ¨¡å‹**æ€»å…±æ£€æµ‹å‡ºçš„ç›®æ ‡æ•°é‡**ï¼ˆåŒ…æ‹¬æ­£ç¡®çš„å’Œé”™è¯¯çš„ï¼‰ã€‚

------

### 3. å¬å›ç‡ (Recall)

- **å®šä¹‰ï¼š** Recall è¡¡é‡çš„æ˜¯æ‰€æœ‰å®é™…å­˜åœ¨çš„ï¼ˆçœŸå®ï¼‰ç›®æ ‡ä¸­ï¼Œæœ‰å¤šå°‘æ¯”ä¾‹**è¢«æ¨¡å‹æˆåŠŸæ£€æµ‹åˆ°**ã€‚å®ƒå…³æ³¨çš„æ˜¯æ¨¡å‹**ä¸æ¼æ£€**çš„èƒ½åŠ›ã€‚

  

  

- **ä½œç”¨ï¼š** å¯¹äºé‚£äº›è¦æ±‚å°½å¯èƒ½å¤šåœ°å‘ç°æ‰€æœ‰ç›®æ ‡çš„ä»»åŠ¡éå¸¸é‡è¦ï¼Œä¾‹å¦‚åŒ»ç–—å›¾åƒè¯Šæ–­ï¼ˆæ¼æ‰ç—…ç¶å¯èƒ½å»¶è¯¯æ²»ç–—ï¼‰æˆ–å®‰é˜²ç›‘æ§ï¼ˆæ¼æ‰å¯ç–‘äººå‘˜ï¼‰ã€‚

- **è®¡ç®—å…¬å¼ï¼š** Recall=TP+FNTP

  - **TP + FN**ï¼šè¡¨ç¤º**æ‰€æœ‰å®é™…å­˜åœ¨çš„çœŸå®ç›®æ ‡æ•°é‡**ã€‚

------

### 4. F1-Score

- å®šä¹‰ï¼š

   F1-Score æ˜¯ Precision å’Œ Recall çš„

  è°ƒå’Œå¹³å‡å€¼

  ã€‚

  

  å®ƒæä¾›äº†ä¸€ä¸ªå•ä¸€çš„åº¦é‡æ¥å¹³è¡¡ Precision å’Œ Recallã€‚å½“ Precision å’Œ Recall éƒ½é«˜æ—¶ï¼ŒF1-Score æ‰ä¼šé«˜ã€‚

- **ä½œç”¨ï¼š** å½“æ‚¨éœ€è¦ä¸€ä¸ªç»¼åˆè¯„ä¼°æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ï¼Œå¹¶ä¸”å¸Œæœ›å…¼é¡¾æŸ¥å‡†ç‡å’ŒæŸ¥å…¨ç‡æ—¶ï¼ŒF1-Score æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©ã€‚

- **è®¡ç®—å…¬å¼ï¼š** F1-Score=2Ã—Precision+RecallPrecisionÃ—Recall

------

### 5. PR æ›²çº¿ (Precision-Recall Curve)

PR æ›²çº¿æ˜¯æè¿° Precision å’Œ Recall ä¹‹é—´å…³ç³»çš„å›¾è¡¨ã€‚



- ç»˜åˆ¶æ–¹æ³•ï¼š
  1. å¯¹æ¨¡å‹è¾“å‡ºçš„æ‰€æœ‰æ£€æµ‹æ¡†æŒ‰ç…§**ç½®ä¿¡åº¦åˆ†æ•°ï¼ˆconfidence scoreï¼‰**è¿›è¡Œé™åºæ’åºã€‚
  2. ä»æœ€é«˜ç½®ä¿¡åº¦çš„æ£€æµ‹æ¡†å¼€å§‹ï¼Œä¾æ¬¡éå†æ¯ä¸ªæ£€æµ‹æ¡†ã€‚
  3. åœ¨æ¯ä¸€æ­¥ï¼Œè®¡ç®—å½“å‰çš„ TPã€FPã€FNï¼Œå¹¶æ›´æ–° Precision å’Œ Recall å€¼ã€‚
  4. ä»¥ Recall ä¸ºæ¨ªè½´ï¼ŒPrecision ä¸ºçºµè½´ï¼Œç»˜åˆ¶ç‚¹å¹¶è¿æ¥ã€‚
- ä½œç”¨ï¼š
  - **å¯è§†åŒ–æƒè¡¡ï¼š** PR æ›²çº¿æ¸…æ™°åœ°å±•ç¤ºäº†æ¨¡å‹åœ¨ä¸åŒç½®ä¿¡åº¦é˜ˆå€¼ä¸‹ Precision å’Œ Recall ä¹‹é—´çš„æƒè¡¡å…³ç³»ã€‚é€šå¸¸ï¼Œæé«˜ Recall ä¼šå¯¼è‡´ Precision ä¸‹é™ï¼Œåä¹‹äº¦ç„¶ã€‚
  - **è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼š** æ›²çº¿è¶Šé è¿‘å³ä¸Šè§’ï¼ˆé«˜ Precision å’Œé«˜ Recallï¼‰ï¼Œè¡¨ç¤ºæ¨¡å‹æ€§èƒ½è¶Šå¥½ã€‚

------

### 6. å¹³å‡ç²¾ç¡®ç‡ (Average Precision, AP)

- **å®šä¹‰ï¼š** AP æ˜¯åœ¨å•ä¸ªç±»åˆ«ä¸Šè¡¡é‡æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ã€‚å®ƒæ˜¯ **PR æ›²çº¿ä¸‹æ–¹çš„é¢ç§¯**ã€‚
- **ä½œç”¨ï¼š** AP æ€»ç»“äº†æ¨¡å‹åœ¨æ‰€æœ‰å¬å›ç‡æ°´å¹³ä¸Šçš„ç²¾ç¡®ç‡ï¼Œä¸ºå•ä¸ªç±»åˆ«çš„æ£€æµ‹æ€§èƒ½æä¾›äº†ä¸€ä¸ªç»¼åˆæ€§çš„åº¦é‡ã€‚AP å€¼è¶Šé«˜ï¼Œè¡¨ç¤ºæ¨¡å‹åœ¨è¯¥ç±»åˆ«ä¸Šçš„æ£€æµ‹æ€§èƒ½è¶Šå¥½ã€‚
- è®¡ç®—æ–¹æ³•ï¼š
  - **æ’å€¼æ³•ï¼ˆå¸¸ç”¨çš„è¿‘ä¼¼è®¡ç®—ï¼Œå¦‚ PASCAL VOC 2007ï¼‰ï¼š** å°† PR æ›²çº¿ä¸Šçš„å¬å›ç‡ç‚¹ (r) ä¸Šçš„ç²¾ç¡®ç‡ (p) æ›¿æ¢ä¸ºè¯¥ç‚¹å³ä¾§ï¼ˆå³å¬å›ç‡æ›´å¤§ï¼‰çš„æœ€å¤§ç²¾ç¡®ç‡ï¼š AP=k=1âˆ‘NPinterp(k)Î”r(k) å…¶ä¸­ Pinterp(k) æ˜¯åœ¨å¬å›ç‡ k å¤„ï¼Œå…¶å³ä¾§æ‰€æœ‰å¬å›ç‡çš„æœ€é«˜ç²¾ç¡®ç‡ï¼›Î”r(k) æ˜¯å¬å›ç‡çš„å˜åŒ–é‡ã€‚ é€šå¸¸ä¼šå– 11 ä¸ªç­‰é—´éš”çš„å¬å›ç‡ç‚¹ (0, 0.1, 0.2, ..., 1.0) å¯¹åº”çš„æœ€å¤§æ’å€¼ç²¾ç¡®ç‡çš„å¹³å‡å€¼æ¥è¿‘ä¼¼ APã€‚
  - **æ‰€æœ‰ç‚¹æ’å€¼æ³•ï¼ˆæ›´ç²¾ç¡®ï¼Œå¦‚ COCOï¼‰ï¼š** è®¡ç®—æ¯ä¸ªç‹¬ç«‹å¬å›ç‡ç‚¹å¯¹åº”çš„ç²¾ç¡®ç‡çš„åŠ æƒå¹³å‡å€¼ï¼Œæƒé‡æ˜¯ç›¸é‚»å¬å›ç‡ä¹‹é—´çš„å˜åŒ–é‡ã€‚è¿™æ›´æ¥è¿‘äºè®¡ç®— PR æ›²çº¿ä¸‹çš„å®é™…é¢ç§¯ã€‚
- **IoU é˜ˆå€¼ï¼š** AP çš„è®¡ç®—å¼ºçƒˆä¾èµ–äºè®¾å®šçš„ IoU é˜ˆå€¼ã€‚ä¾‹å¦‚ï¼ŒAP0.5 è¡¨ç¤ºåœ¨ IoU é˜ˆå€¼ä¸º 0.5 æ—¶è®¡ç®—çš„ APã€‚

------

### 7. å¹³å‡å¬å›ç‡ (Average Recall, AR)

- **å®šä¹‰ï¼š** AR æ˜¯åœ¨ç»™å®šçš„ IoU é˜ˆå€¼å’Œæ¯ä¸ªå›¾åƒå›ºå®šæ•°é‡çš„æ£€æµ‹ç»“æœä¸‹ï¼Œæ‰€æœ‰ç±»åˆ«çš„å¹³å‡æœ€å¤§å¬å›ç‡ã€‚
- **ä½œç”¨ï¼š** AR é€šå¸¸åœ¨ COCO ç­‰è¯„ä¼°æ ‡å‡†ä¸­ä½¿ç”¨ï¼Œå®ƒå¼ºè°ƒæ¨¡å‹åœ¨ä¸è€ƒè™‘ç²¾ç¡®ç‡çš„æƒ…å†µä¸‹ï¼Œåœ¨ç‰¹å®šæ•°é‡çš„æ£€æµ‹æ¡†é™åˆ¶ä¸‹ï¼Œèƒ½å¤Ÿå¬å›å¤šå°‘çœŸå®ç›®æ ‡ã€‚ä¾‹å¦‚ï¼ŒAR100 è¡¨ç¤ºåœ¨æ¯å¼ å›¾åªå–ç½®ä¿¡åº¦æœ€é«˜çš„ 100 ä¸ªæ£€æµ‹æ¡†çš„æƒ…å†µä¸‹ï¼Œè®¡ç®—çš„å¹³å‡å¬å›ç‡ã€‚
- **è®¡ç®—æ–¹æ³•ï¼š** é€šå¸¸æ˜¯åœ¨å¤šä¸ª IoU é˜ˆå€¼ä¸‹ï¼ˆä¾‹å¦‚ï¼ŒCOCO åœ¨ [0.5, 0.95] ä¹‹é—´ä»¥ 0.05 æ­¥é•¿å– 10 ä¸ªé˜ˆå€¼ï¼‰ï¼Œè®¡ç®—ç»™å®šæœ€å¤§æ£€æµ‹æ¡†æ•°é‡ä¸‹çš„å¹³å‡å¬å›ç‡ã€‚

------

### 8. å¹³å‡ç²¾åº¦å‡å€¼ (Mean Average Precision, mAP)

- **å®šä¹‰ï¼š** mAP æ˜¯åœ¨**æ‰€æœ‰ç›®æ ‡ç±»åˆ«ä¸Š**çš„ AP çš„å¹³å‡å€¼ã€‚

- ä½œç”¨ï¼š

   mAP æ˜¯ç›®æ ‡æ£€æµ‹é¢†åŸŸæœ€å¸¸ç”¨å’Œæœ€é‡è¦çš„ç»¼åˆè¯„ä¼°æŒ‡æ ‡ã€‚

  å®ƒèƒ½å¤Ÿå…¨é¢åæ˜ æ¨¡å‹åœ¨æ‰€æœ‰ç±»åˆ«ä¸Šçš„æ£€æµ‹æ€§èƒ½ã€‚mAP å€¼è¶Šé«˜ï¼Œè¯´æ˜æ¨¡å‹åœ¨æ£€æµ‹æ‰€æœ‰ç±»å‹çš„ç›®æ ‡æ–¹é¢è¡¨ç°è¶Šå¥½ã€‚

- **è®¡ç®—å…¬å¼ï¼š** mAP=Nclasses1i=1âˆ‘NclassesAPi å…¶ä¸­ Nclasses æ˜¯ç›®æ ‡ç±»åˆ«çš„æ€»æ•°ï¼ŒAPi æ˜¯ç¬¬ i ä¸ªç±»åˆ«çš„ APã€‚

- IoU é˜ˆå€¼ï¼š

   å’Œ AP ä¸€æ ·ï¼ŒmAP ä¹Ÿä¾èµ–äº IoU é˜ˆå€¼ã€‚

  - **PASCAL VOC è¯„ä¼°ï¼š** é€šå¸¸åªè®¡ç®—åœ¨ IoU é˜ˆå€¼ä¸º 0.5 æ—¶çš„ mAPï¼Œè®°ä½œ mAP0.5ã€‚
  - **COCO è¯„ä¼°ï¼š** æ›´ä¸¥æ ¼ï¼Œå®ƒè®¡ç®—åœ¨ 10 ä¸ªä¸åŒçš„ IoU é˜ˆå€¼ä¸‹ï¼ˆä» 0.5 åˆ° 0.95ï¼Œæ­¥é•¿ä¸º 0.05ï¼‰çš„ AP çš„å¹³å‡å€¼ï¼Œç„¶åå°†è¿™ 10 ä¸ªå¹³å‡ AP å†æ¬¡å¹³å‡ï¼Œå¾—åˆ°æœ€ç»ˆçš„ mAP å€¼ã€‚è®°ä½œ mAP@[.5:.05:.95] æˆ– mAP0.5:0.95ã€‚è¿™ä½¿å¾— COCO çš„è¯„ä¼°æ›´å…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒè¦æ±‚æ¨¡å‹åœ¨ä¸åŒå®šä½ç²¾åº¦è¦æ±‚ä¸‹éƒ½è¡¨ç°è‰¯å¥½ã€‚

------

### 9. ROC æ›²çº¿ (Receiver Operating Characteristic Curve) å’Œ AUC (Area Under the Curve)

ROC æ›²çº¿å’Œ AUC æ›´å¤šåœ°ç”¨äº**äºŒåˆ†ç±»é—®é¢˜**ï¼ˆåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€åƒç´ çº§åˆ«çš„è¯­ä¹‰åˆ†å‰²ä¸­çš„å‰æ™¯/èƒŒæ™¯åˆ†ç±»ï¼‰ï¼Œåœ¨ä¼ ç»Ÿçš„ç›®æ ‡æ£€æµ‹ä¸­ï¼Œå®ƒä»¬ä¸æ˜¯ä¸»æµçš„è¯„ä¼°æŒ‡æ ‡ï¼Œå› ä¸ºç›®æ ‡æ£€æµ‹æ˜¯**å¤šå®ä¾‹ã€å¤šç±»åˆ«ã€ä»¥åŠåŒ…å«å®šä½çš„å¤æ‚ä»»åŠ¡**ã€‚ç„¶è€Œï¼Œäº†è§£å®ƒä»¬å¯¹äºç†è§£åˆ†ç±»æ¨¡å‹çš„æ€§èƒ½ä»ç„¶å¾ˆé‡è¦ã€‚

- **çœŸé˜³æ€§ç‡ (True Positive Rate, TPR) / å¬å›ç‡ (Recall)ï¼š** TPR=TP+FNTP
- **å‡é˜³æ€§ç‡ (False Positive Rate, FPR)ï¼š** FPR=FP+TNFP
- **ROC æ›²çº¿ï¼š**
  - **å®šä¹‰ï¼š** ä»¥ FPR ä¸ºæ¨ªè½´ï¼ŒTPR ä¸ºçºµè½´ç»˜åˆ¶çš„æ›²çº¿ã€‚é€šè¿‡æ”¹å˜åˆ†ç±»å™¨çš„åˆ†ç±»é˜ˆå€¼ï¼ˆå¯¹äºç›®æ ‡æ£€æµ‹å¯ä»¥ç†è§£ä¸ºç½®ä¿¡åº¦é˜ˆå€¼ï¼‰ï¼Œå¯ä»¥å¾—åˆ°ä¸€ç³»åˆ—çš„ (FPR, TPR) ç‚¹ã€‚
  - **ä½œç”¨ï¼š** æ˜¾ç¤ºäº†åœ¨ä¸åŒåˆ†ç±»é˜ˆå€¼ä¸‹ï¼Œåˆ†ç±»å™¨å¯¹æ­£ç±»å’Œè´Ÿç±»çš„åŒºåˆ†èƒ½åŠ›ã€‚
  - **æ›²çº¿ç‰¹æ€§ï¼š** æ›²çº¿è¶Šé è¿‘å·¦ä¸Šè§’ï¼ˆFPR æ¥è¿‘ 0ï¼ŒTPR æ¥è¿‘ 1ï¼‰ï¼Œæ¨¡å‹æ€§èƒ½è¶Šå¥½ã€‚å¯¹è§’çº¿è¡¨ç¤ºéšæœºåˆ†ç±»å™¨ã€‚
- **AUC (Area Under the ROC Curve)ï¼š**
  - **å®šä¹‰ï¼š** ROC æ›²çº¿ä¸‹æ–¹çš„é¢ç§¯ã€‚
  - **ä½œç”¨ï¼š** AUC æä¾›äº†ä¸€ä¸ªå•ä¸€çš„æ•°å€¼æ¥é‡åŒ– ROC æ›²çº¿çš„æ€§èƒ½ã€‚AUC å€¼åœ¨ 0 åˆ° 1 ä¹‹é—´ï¼Œå€¼è¶Šå¤§è¡¨ç¤ºæ¨¡å‹åŒºåˆ†æ­£è´Ÿæ ·æœ¬çš„èƒ½åŠ›è¶Šå¼ºã€‚
  - **è®¡ç®—æ–¹æ³•ï¼š** æ•°å­¦ä¸Šæ˜¯è®¡ç®— ROC æ›²çº¿ä¸‹çš„å®šç§¯åˆ†ã€‚å®é™…ä¸­é€šå¸¸é€šè¿‡å¯¹æ›²çº¿ä¸‹çš„æ¢¯å½¢é¢ç§¯æ±‚å’Œæ¥è¿‘ä¼¼ã€‚
  - ä¸ºä»€ä¹ˆåœ¨ç›®æ ‡æ£€æµ‹ä¸­è¾ƒå°‘ç”¨ï¼š
    1. **TN çš„å®šä¹‰é—®é¢˜ï¼š** åœ¨ç›®æ ‡æ£€æµ‹ä¸­ï¼ŒèƒŒæ™¯åŒºåŸŸè¿‡äºåºå¤§ï¼ŒTN çš„è®¡ç®—éå¸¸å›°éš¾ä¸”é€šå¸¸æ²¡æœ‰æ„ä¹‰ã€‚
    2. **ä¸å…³æ³¨å®šä½ï¼š** ROC å’Œ AUC ä¸»è¦æ˜¯ä¸ºäº†è¯„ä¼°åˆ†ç±»æ€§èƒ½ï¼Œå®ƒä»¬ä¸ç›´æ¥è€ƒè™‘æ£€æµ‹æ¡†çš„å®šä½ç²¾åº¦ï¼ˆIoUï¼‰ï¼Œè€Œè¿™æ˜¯ç›®æ ‡æ£€æµ‹çš„æ ¸å¿ƒã€‚
    3. **å¤šå®ä¾‹é—®é¢˜ï¼š** ä¸€å¼ å›¾åƒä¸­å¯èƒ½åŒ…å«å¤šä¸ªç›¸åŒæˆ–ä¸åŒç±»åˆ«çš„ç›®æ ‡ï¼Œè¿™ä½¿å¾—å°†äºŒåˆ†ç±»çš„ ROC/AUC ç›´æ¥æ¨å¹¿åˆ°ç›®æ ‡æ£€æµ‹å˜å¾—å¤æ‚ã€‚

------

### æ€»ç»“

è¿™äº›æŒ‡æ ‡å…±åŒä¸ºæˆ‘ä»¬æä¾›äº†å…¨é¢è¯„ä¼°ç›®æ ‡æ£€æµ‹æ¨¡å‹æ€§èƒ½çš„å·¥å…·ï¼š

- **Precision, Recall, F1-Scoreï¼š** æä¾›åœ¨ç‰¹å®šç½®ä¿¡åº¦é˜ˆå€¼ä¸‹ï¼Œæ¨¡å‹æŸ¥å‡†ç‡å’ŒæŸ¥å…¨ç‡çš„ç›´è§‚è¡¨ç°ã€‚

- AP, mAPï¼š

   æ˜¯ç›®æ ‡æ£€æµ‹ä¸­æœ€æ ¸å¿ƒçš„æŒ‡æ ‡ï¼Œå®ƒä»¬ç»¼åˆè€ƒè™‘äº†æ¨¡å‹åœ¨ä¸åŒå¬å›ç‡æ°´å¹³ä¸Šçš„ç²¾ç¡®ç‡ä»¥åŠä¸åŒ IoU é˜ˆå€¼ä¸‹çš„å®šä½èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

- ARï¼š

   è¡¥å……æ€§æŒ‡æ ‡ï¼Œå…³æ³¨åœ¨æœ‰é™æ£€æµ‹æ¡†æ•°é‡ä¸‹çš„å¬å›èƒ½åŠ›ã€‚

- **PR æ›²çº¿ï¼š** ç›´è§‚å±•ç¤º Precision å’Œ Recall ä¹‹é—´çš„æƒè¡¡ã€‚

- **ROC/AUCï¼š** è™½ç„¶åœ¨ä¼ ç»Ÿç›®æ ‡æ£€æµ‹ä¸­è¾ƒå°‘ç›´æ¥ä½¿ç”¨ï¼Œä½†å¯¹äºç†è§£åˆ†ç±»æ¨¡å‹çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚

åœ¨å®é™…è¯„ä¼°ä¸­ï¼Œé€‰æ‹©å“ªäº›æŒ‡æ ‡ä»¥åŠä½¿ç”¨å“ªä¸ª IoU é˜ˆå€¼é€šå¸¸å–å†³äºå…·ä½“çš„åº”ç”¨åœºæ™¯å’Œè¯„ä¼°æ ‡å‡†ï¼ˆä¾‹å¦‚ PASCAL VOCã€COCOã€Open Images ç­‰ï¼‰ã€‚

# æ¢¯åº¦

- **æ¢¯åº¦æ¶ˆå¤±**ï¼šæ¢¯åº¦å€¼è¶Šæ¥è¶Šå°ï¼ˆè¶‹è¿‘äº 0ï¼‰ï¼Œå¯¼è‡´æµ…å±‚å‚æ•°å‡ ä¹ä¸æ›´æ–°ã€‚
- **æ¢¯åº¦çˆ†ç‚¸**ï¼šæ¢¯åº¦å€¼è¶Šæ¥è¶Šå¤§ï¼Œå¯¼è‡´å‚æ•°æ›´æ–°æ­¥é•¿è¿‡å¤§ï¼Œæ¨¡å‹å‘æ•£ã€‚



## éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆStochastic Gradient Descent, SGDï¼‰

#### **2. ç®—æ³•åŸç†**

ç»™å®šè®­ç»ƒé›† 
$$
\{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\}
$$
ç›®æ ‡æ˜¯æœ€å°åŒ–æŸå¤±å‡½æ•° 
$$
J(\theta) = \frac{1}{n} \sum_{i=1}^n L(f(x_i; \theta), y_i)
$$
**SGD çš„å‚æ•°æ›´æ–°å…¬å¼**ï¼š
$$
\theta = \theta - \eta \cdot \nabla L(f(x_i; \theta), y_i)
$$


- $\eta$ æ˜¯å­¦ä¹ ç‡ï¼ˆæ§åˆ¶æ­¥é•¿ï¼‰
- $\nabla L$ æ˜¯å•ä¸ªæ ·æœ¬çš„æŸå¤±å‡½æ•°æ¢¯åº¦
- æ¯æ¬¡è¿­ä»£éšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬ $(x_i, y_i)$

| **ç‰¹æ€§**         | **æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆBGDï¼‰**    | **éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰**    |
| ---------------- | -------------------------- | -------------------------- |
| **æ¢¯åº¦è®¡ç®—**     | ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ ·æœ¬           | ä½¿ç”¨å•ä¸ªæ ·æœ¬æˆ–å°æ‰¹é‡       |
| **è®¡ç®—æ•ˆç‡**     | ä½ï¼ˆå°¤å…¶å¤§æ•°æ®é›†ï¼‰         | é«˜ï¼ˆæ¯æ¬¡ä»…éœ€ä¸€ä¸ªæ ·æœ¬ï¼‰     |
| **å†…å­˜éœ€æ±‚**     | é«˜ï¼ˆéœ€å­˜å‚¨æ‰€æœ‰æ ·æœ¬ï¼‰       | ä½ï¼ˆä»…éœ€ä¸€ä¸ªæ ·æœ¬ï¼‰         |
| **æ”¶æ•›æ€§**       | ç¨³å®šä¸‹é™ï¼Œå¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜ | æ³¢åŠ¨å¤§ï¼Œä½†å¯èƒ½è·³å‡ºå±€éƒ¨æœ€ä¼˜ |
| **å‚æ•°æ›´æ–°æ–¹å‘** | å…¨å±€æœ€ä¼˜æ–¹å‘               | éšæœºæ³¢åŠ¨ï¼Œæ€»ä½“æœå‘æœ€ä¼˜è§£   |



## SGDæ”¹è¿›

ä¸ºå…‹æœ SGD çš„ç¼ºé™·ï¼Œè¡ç”Ÿå‡ºä»¥ä¸‹ä¼˜åŒ–ç®—æ³•ï¼š

##### **ï¼ˆ1ï¼‰å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆMini-Batch SGDï¼‰**

- æ¯æ¬¡ä½¿ç”¨ **m ä¸ªæ ·æœ¬**ï¼ˆå¦‚ 32ã€64ï¼‰è®¡ç®—æ¢¯åº¦ï¼š $\theta = \theta - \eta \cdot \frac{1}{m} \sum_{i \in \mathcal{B}} \nabla L(f(x_i; \theta), y_i)$
- å¹³è¡¡äº†è®¡ç®—æ•ˆç‡å’Œæ¢¯åº¦ç¨³å®šæ€§ï¼Œæ˜¯æ·±åº¦å­¦ä¹ æœ€å¸¸ç”¨çš„æ–¹æ³•ã€‚

##### **ï¼ˆ2ï¼‰å¸¦åŠ¨é‡çš„ SGDï¼ˆMomentumï¼‰**

- å¼•å…¥åŠ¨é‡é¡¹åŠ é€Ÿæ”¶æ•›ï¼Œå‡å°‘éœ‡è¡ï¼š

  $v_t = \gamma v_{t-1} + \eta \cdot \nabla L(f(x_i; \theta), y_i)$ 

  $\theta = \theta - v_t$

- $\gamma$ æ˜¯åŠ¨é‡ç³»æ•°ï¼ˆé€šå¸¸å– 0.9ï¼‰ï¼Œç±»ä¼¼ â€œæƒ¯æ€§â€ã€‚

##### **ï¼ˆ3ï¼‰Adagrad**

- è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡ï¼Œå¯¹é¢‘ç¹æ›´æ–°çš„å‚æ•°ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡ï¼š 

  $g_t = \nabla L(f(x_i; \theta), y_i)$

  $\theta = \theta - \frac{\eta}{\sqrt{G_t + \epsilon}} \odot g_t$

- $G_t$ç´¯ç§¯æ¢¯åº¦å¹³æ–¹å’Œï¼Œ$\epsilon$é˜²æ­¢åˆ†æ¯ä¸ºé›¶ã€‚

##### **ï¼ˆ4ï¼‰Adam**

- ç»“åˆåŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡ï¼š

  $m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t \quad \text{ï¼ˆä¸€é˜¶çŸ©ä¼°è®¡ï¼Œå³åŠ¨é‡ï¼‰}$

  $v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2 \quad \text{ï¼ˆäºŒé˜¶çŸ©ä¼°è®¡ï¼Œå³æ¢¯åº¦å¹³æ–¹ï¼‰}$

   $\theta = \theta - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$

- å¸¸ç”¨å‚æ•°ï¼š$\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=10^{-8}$ã€‚



## Weight Decayï¼ˆæƒé‡è¡°å‡ï¼‰ï¼šæ·±åº¦å­¦ä¹ ä¸­çš„æ­£åˆ™åŒ–æŠ€æœ¯

Weight decayï¼ˆæƒé‡è¡°å‡ï¼‰æ˜¯ä¸€ç§å¸¸ç”¨çš„**æ­£åˆ™åŒ–æ–¹æ³•**ï¼Œç”¨äºé˜²æ­¢ç¥ç»ç½‘ç»œè¿‡æ‹Ÿåˆã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ ä¸€ä¸ª**æƒ©ç½šé¡¹**ï¼ŒæŠ‘åˆ¶æ¨¡å‹å‚æ•°ï¼ˆæƒé‡ï¼‰è¿‡å¤§ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

åœ¨æ ‡å‡†æŸå¤±å‡½æ•° \(L(\theta)\) ä¸­åŠ å…¥ **L2 æ­£åˆ™åŒ–é¡¹**ï¼š

 $L'(\theta) = L(\theta) + \frac{\lambda}{2} \sum_{w \in \theta} w^2$ å…¶ä¸­ï¼š

- $\theta$ æ˜¯æ¨¡å‹å‚æ•°ï¼ˆæƒé‡ï¼‰
- $\lambda$æ˜¯æ­£åˆ™åŒ–å¼ºåº¦ï¼ˆè¶…å‚æ•°ï¼Œé€šå¸¸ä¸º $10^{-4}$é‡çº§ï¼‰
- $\frac{\lambda}{2} \sum_{w} w^2$æ˜¯ L2 èŒƒæ•°çš„å¹³æ–¹

**æƒé‡è¡°å‡çš„ä½œç”¨**

1. **é™åˆ¶æ¨¡å‹å¤æ‚åº¦**ï¼šé€šè¿‡æƒ©ç½šå¤§æƒé‡ï¼Œä½¿æ¨¡å‹æ›´ â€œå¹³æ»‘â€ï¼Œå‡å°‘å¯¹è®­ç»ƒæ•°æ®çš„è¿‡æ‹Ÿåˆã€‚
2. **ç¼“è§£æ¢¯åº¦æ¶ˆå¤± / çˆ†ç‚¸**ï¼šé€‚å½“çš„æƒé‡è¡°å‡æœ‰åŠ©äºä¿æŒæƒé‡åœ¨åˆç†èŒƒå›´å†…ï¼Œç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚
3. **æé«˜æ³›åŒ–èƒ½åŠ›**ï¼šè¿«ä½¿æ¨¡å‹å…³æ³¨æœ€æ˜¾è‘—çš„ç‰¹å¾ï¼Œå¿½ç•¥å™ªå£°ã€‚

 **æƒé‡è¡°å‡ä¸ L2 æ­£åˆ™åŒ–çš„å…³ç³»**

åœ¨å¤§å¤šæ•°ä¼˜åŒ–ç®—æ³•ä¸­ï¼ˆå¦‚ SGDã€Adamï¼‰ï¼Œ**æƒé‡è¡°å‡ä¸ L2 æ­£åˆ™åŒ–åœ¨æ•°å­¦ä¸Šç­‰ä»·**ã€‚ä»¥ SGD ä¸ºä¾‹ï¼š

**æ ‡å‡† SGD æ›´æ–°**ï¼š $w = w - \eta \cdot \frac{\partial L}{\partial w}$

**åŠ å…¥ L2 æ­£åˆ™åŒ–åçš„æ›´æ–°**ï¼š 

$w = w - \eta \left( \frac{\partial L}{\partial w} + \lambda w \right)$ 

$\Rightarrow w = (1 - \eta \lambda)w - \eta \cdot \frac{\partial L}{\partial w}$

**æƒé‡è¡°å‡çš„æ•ˆæœ**ï¼šæ¯æ¬¡æ›´æ–°æ—¶ï¼Œæƒé‡å…ˆä¹˜ä»¥ä¸€ä¸ªå°äº 1 çš„ç³»æ•° $(1 - \eta \lambda)$ï¼Œå†è¿›è¡Œæ¢¯åº¦æ›´æ–°ï¼Œå› æ­¤ç§°ä¸º â€œæƒé‡è¡°å‡â€ã€‚

æƒé‡è¡°å‡å¯¹æ¨¡å‹è®­ç»ƒçš„å½±å“ï¼š

- **æ— æƒé‡è¡°å‡**ï¼šè®­ç»ƒæŸå¤±æŒç»­ä¸‹é™ï¼Œä½†éªŒè¯æŸå¤±å¯èƒ½åœ¨æŸç‚¹åå¼€å§‹ä¸Šå‡ï¼ˆè¿‡æ‹Ÿåˆï¼‰ã€‚
- **æœ‰æƒé‡è¡°å‡**ï¼šè®­ç»ƒæŸå¤±ä¸‹é™é€Ÿåº¦ç¨æ…¢ï¼Œä½†éªŒè¯æŸå¤±æ›´ä½ä¸”æ›´ç¨³å®šï¼ˆæ³›åŒ–èƒ½åŠ›æ›´å¼ºï¼‰ã€‚



# ä¸Šé‡‡æ ·

## ğŸ”¹ 1. æœ€è¿‘é‚»æ’å€¼ï¼ˆNearest Neighborï¼‰

ç”¨æœ€è¿‘çš„åƒç´ å€¼ç›´æ¥å¤åˆ¶åˆ°æ‰©å±•åŒºåŸŸä¸­ï¼Œé€Ÿåº¦å¿«ä½†ä¼šå‡ºç°é”¯é½¿æ„Ÿã€‚

```
åŸå›¾ (2x2)         ä¸Šé‡‡æ ·ä¸º (4x4)
+----+----+        +----+----+----+----+
| A  | B  |        | A  | A  | B  | B  |
+----+----+   =>   +----+----+----+----+
| C  | D  |        | C  | C  | D  | D  |
+----+----+        +----+----+----+----+
```

ğŸ› ï¸ PyTorch ç¤ºä¾‹ï¼š

```python
import torch.nn.functional as F

upsampled = F.interpolate(input, scale_factor=2, mode='nearest')
```

------

## ğŸ”¹ 2. åŒçº¿æ€§æ’å€¼ï¼ˆBilinear Interpolationï¼‰

å¯¹ 4 ä¸ªé‚»è¿‘åƒç´ åšåŠ æƒå¹³å‡ï¼Œæ•ˆæœæ¯”æœ€è¿‘é‚»å¹³æ»‘ï¼Œä½†ä¹Ÿå¯èƒ½ä¸¢ç»†èŠ‚ã€‚

```
           +----+----+
           | A  | B  |
           +----+----+   â†’ æ’å€¼åä¸­å¿ƒåƒç´ æ˜¯ A~D çš„åŠ æƒå¹³å‡
           | C  | D  |
           +----+----+
```

ğŸ› ï¸ PyTorch ç¤ºä¾‹ï¼š

```python
upsampled = F.interpolate(input, scale_factor=2, mode='bilinear', align_corners=True)
```

------

## ğŸ”¹ 3. è½¬ç½®å·ç§¯ï¼ˆTransposed Convolution / Deconvolutionï¼‰

é€šè¿‡â€œåæ–¹å‘â€çš„å·ç§¯æ¥æ¢å¤ç©ºé—´å°ºå¯¸ï¼Œ**å‚æ•°å¯å­¦ä¹ **ï¼Œå¸¸ç”¨äºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œè¯­ä¹‰åˆ†å‰²ã€‚

```
è¾“å…¥ 2Ã—2ï¼Œç»è¿‡ stride=2 çš„è½¬ç½®å·ç§¯ => è¾“å‡º 4Ã—4

åå·ç§¯å®é™…æ˜¯åœ¨æ¯ä¸ªè¾“å…¥åƒç´ ä¹‹é—´æ’å…¥ç©ºç™½ï¼Œå†å·ç§¯
```

ğŸ› ï¸ PyTorch ç¤ºä¾‹ï¼š

```python
import torch.nn as nn

deconv = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1)
output = deconv(input)
```

ğŸ“Œ **æ³¨æ„äº‹é¡¹**ï¼š

- è½¬ç½®å·ç§¯å¯èƒ½å‡ºç° checkerboard artifactï¼ˆæ£‹ç›˜æ ¼å™ªå£°ï¼‰
- `output_padding` ç”¨äºæ§åˆ¶è¾“å‡ºå°ºå¯¸å¯¹é½

------

## ğŸ”¹ 4. Pixel Shuffleï¼ˆäºšåƒç´ å·ç§¯ï¼‰

æŠŠå·ç§¯ç»“æœçš„é€šé“ç»´åº¦â€œæ‰“æ•£â€æˆç©ºé—´å°ºå¯¸ã€‚éå¸¸é€‚åˆç”¨äºå›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡ã€‚

```
è¾“å…¥ç»´åº¦ï¼š [B, CÃ—rÂ², H, W] â†’ è¾“å‡ºç»´åº¦ï¼š [B, C, HÃ—r, WÃ—r]
é€šé“åƒç´ é‡æ’ï¼ŒæŠŠé€šé“å¡è¿›ç©ºé—´é‡Œ
```

ğŸ› ï¸ PyTorch ç¤ºä¾‹ï¼š

```python
import torch.nn as nn

pixel_shuffle = nn.PixelShuffle(upscale_factor=2)
output = pixel_shuffle(input)  # input.shape = (B, C*4, H, W)
```

------

## ğŸ”¹ 5. Unpoolingï¼ˆåæ± åŒ–ï¼‰

æ˜¯æœ€å¤§æ± åŒ–çš„â€œé€†æ“ä½œâ€ï¼Œç”¨æ± åŒ–æ—¶è®°å½•çš„ç´¢å¼•æ¥è¿˜åŸä½ç½®ä¿¡æ¯ã€‚

```
MaxPool:  â†“ â†“ â†“            Unpooling: â†‘ â†‘ â†‘
Input:   1 3 2  â†’ MaxPool: 3     â†’ Unpooling: 0 3 0
         2 1 0             Index: 1                  (3 came from index 1)
```

ğŸ› ï¸ PyTorch ç¤ºä¾‹ï¼š

```python
pool = nn.MaxPool2d(2, stride=2, return_indices=True)
unpool = nn.MaxUnpool2d(2, stride=2)

pooled, indices = pool(input)
restored = unpool(pooled, indices)
```

------

## æ€»ç»“è¡¨æ ¼å¯¹æ¯”

| æ–¹æ³•       | å¯å­¦ä¹  | æ˜¯å¦ä¿ç•™ç»†èŠ‚ | æ˜¯å¦å¸¸ç”¨äºè®­ç»ƒ | æ˜¯å¦æ˜“å®ç° |
| ---------- | ------ | ------------ | -------------- | ---------- |
| æœ€è¿‘é‚»æ’å€¼ | å¦     | âŒ            | å¦             | âœ…          |
| åŒçº¿æ€§æ’å€¼ | å¦     | â­•            |                |            |

ä¸­ç­‰

| å¦             | âœ…         |
 | è½¬ç½®å·ç§¯         | âœ…     | âœ…ï¼ˆå¯å­¦ä¹ ï¼‰ | âœ…             | ä¸­ç­‰       |
 | Pixel Shuffle    | âœ…     | âœ…           | âœ…ï¼ˆè¶…åˆ†ï¼‰     | ä¸­ç­‰       |
 | åæ± åŒ–ï¼ˆUnpoolï¼‰ | å¦     | âœ…ï¼ˆåŸºäºç´¢å¼•ï¼‰| âœ…ï¼ˆé…åˆæ± åŒ–ï¼‰ | ä¸­ç­‰       |

| ä»»åŠ¡              | æ¨èä¸Šé‡‡æ ·æ–¹æ³•                   |
| ----------------- | -------------------------------- |
| GANï¼ˆå›¾åƒç”Ÿæˆï¼‰   | è½¬ç½®å·ç§¯ / Pixel Shuffle         |
| è¯­ä¹‰åˆ†å‰²ï¼ˆU-Netï¼‰ | è½¬ç½®å·ç§¯ + Skip Connection       |
| å›¾åƒè¶…åˆ†è¾¨ç‡      | Pixel Shuffleï¼ˆEDSR / SRGAN ç­‰ï¼‰ |
| è½»é‡æ¨ç†ä»»åŠ¡      | æœ€è¿‘é‚» / åŒçº¿æ€§æ’å€¼ï¼ˆæ•ˆç‡é«˜ï¼‰    |

# å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰

> [!NOTE]
>
> **è¾“å…¥ â†’ å·ç§¯å±‚ â†’ ReLU â†’ æ± åŒ– â†’ Flatten â†’ å…¨è¿æ¥ â†’ Softmax â†’ æŸå¤±å‡½æ•°**

## å·ç§¯

```
nn.Conv2d(1,6,kernel_size=5,padding=2),
nn.ReLU(),
```

å·ç§¯ä¸­å¡«å……æ›´å¸¸è§

å‡è®¾è¾“å…¥æ˜¯ä¸€ä¸ª32\*32\*3çš„å½©è‰²å›¾åƒï¼Œå·ç§¯æ ¸å¾—æ˜¯3é€šé“çš„ï¼Œ**åŒæ—¶å¯¹ä¸‰ä¸ªé€šé“åšå·ç§¯æ“ä½œï¼Œæœ€åæŠŠä¸‰ä¸ªé€šé“åŠ åœ¨ä¸€èµ·ï¼Œè¾“å‡ºä¸€ä¸ªå€¼ã€‚**

å¯ä»¥æœ‰å¤šä¸ªå·ç§¯æ ¸   å·ç§¯å±‚åï¼Œ**è¾“å‡ºé€šé“ç­‰äºå·ç§¯æ ¸ä¸ªæ•°**ï¼Œä¹Ÿå°±æ˜¯å‡ å¼ ç‰¹å¾å›¾
$$
Output_{size}=\frac{W-F+2P}{S}+1
$$
å‡å¦‚æˆ‘ä½¿ç”¨äº†ä¸¤ä¸ªå·ç§¯æ ¸ï¼Œè¾“å‡ºçš„é€šé“æ•°å°±æ˜¯2ä¸ªï¼Œæ–°ä¸€å±‚å·ç§¯æ ¸å°ºå¯¸ï¼š3\*3\*2,è¿™å±‚å¯ä»¥ä½¿ç”¨64ä¸ªå·ç§¯æ ¸ï¼Œä¸‹ä¸€å±‚é€šé“æ•°å°±æœ‰64ä¸ª

**å·ç§¯ä¹‹åéœ€è¦åŠ æ¿€æ´»å‡½æ•°**

![image-20250514205220037](C:\Users\Anna\AppData\Roaming\Typora\typora-user-images\image-20250514205220037.png)



å·ç§¯ä¸¾ä¾‹ï¼š

![image-20250505103256889](C:\Users\Anna\AppData\Roaming\Typora\typora-user-images\image-20250505103256889.png)

![image-20250505103313597](C:\Users\Anna\AppData\Roaming\Typora\typora-user-images\image-20250505103313597.png)

<img src="C:\Users\Anna\AppData\Roaming\Typora\typora-user-images\image-20250505103326774.png" alt="image-20250505103326774" style="zoom:50%;" />

## æ± åŒ–

æ± åŒ–ï¼š**ä¸‹é‡‡æ ·**ï¼ˆdown-samplingï¼‰ ç¼©å°å›¾åƒ

é€šé“æ•°ä¸å˜

ä¸åŒæ± åŒ–æ–¹å¼ï¼š

â€¢ æœ€å¤§æ± åŒ–

â€¢ å¹³å‡æ± åŒ–

â€¢ åŒçº¿æ€§æ’å€¼

è¾“å…¥å°ºå¯¸ï¼šW*H

æ± åŒ–çª—å£ï¼šF*F

æ­¥é•¿ï¼šS

å¡«å……ï¼šp(ä¸ºä»€ä¹ˆè¦å¡«å……ï¼Ÿè¾“å…¥5\*5ï¼Œæ± åŒ–å°ºå¯¸2\*2,ï¼Œæ˜¾ç„¶ä¸èƒ½æ•´é™¤)
$$
Output_{size}=[\frac{W-F+2P}{S}]+1
$$

### AlexNet ç¬¬ä¸€å±‚

![image-20250515131158756](C:\Users\Anna\AppData\Roaming\Typora\typora-user-images\image-20250515131158756.png)

âœ” è¿™é‡Œ `âŒŠ54.25âŒ‹ = 54`ï¼Œæ‰€ä»¥åŠ  1 åå¾—åˆ° 55ã€‚



## å±•å¹³

æŠŠä¸‰ç»´å¼ é‡è½¬ä¸ºä¸€ç»´ï¼š

ä¾‹å¦‚å°ºå¯¸ä¸º8\*8\*128å±•æˆä¸€ç»´ä¸º8\*8\*128=8192ï¼Œ[x1,x2,......],äº¤ç»™å…¨è¿æ¥å±‚å¤„ç†

## å…¨è¿æ¥å±‚

åšé€»è¾‘åˆ¤æ–­ï¼Œè¾“å‡ºåˆ†ç±»ç»“æœï¼šä¾‹å¦‚ä½¿ç”¨softmax

## å·ç§¯åå‘ä¼ æ’­







## torchçš„ä¸€äº›ç”¨æ³•

```
x = torch.randn(32, 3, 28, 28)
```

è¿™è¡¨ç¤ºï¼š

- æœ‰ 32 å¼ å›¾ â†’ **batch size = 32**

- æ¯å¼ å›¾æœ‰ 3 ä¸ªé€šé“ï¼ˆRGBï¼‰

- å›¾åƒå¤§å°æ˜¯ 28Ã—28

- å‡è®¾ä½ æœ‰ 1000 å¼ çŒ«çŒ«å›¾ç‰‡è¦è®­ç»ƒä¸€ä¸ªè¯†åˆ«æ¨¡å‹ï¼š

  - å¦‚æœ `batch_size = 1`ï¼šæ¯æ¬¡åªå–‚ 1 å¼ å›¾ â†’ **å¤ªæ…¢ï¼è®­ç»ƒä¸ç¨³å®š**
  - å¦‚æœ `batch_size = 1000`ï¼šæ¯æ¬¡å–‚æ•´æ‰¹ â†’ **çˆ†å†…å­˜ï¼æ˜¾å¡è¯´ç½¢å·¥äº†**
  - å¦‚æœ `batch_size = 64`ï¼šæ¯æ¬¡å–‚ 64 å¼  â†’ **é€Ÿåº¦å¿«ï¼Œè®­ç»ƒç¨³**

  æ‰€ä»¥å¸¸è§è®¾ç½®æœ‰ï¼š16ã€32ã€64ã€128ï¼ˆæ ¹æ® GPU å¤§å°æ¥é€‰ï¼‰

å¥½è€¶ï½ç°åœ¨æˆ‘ä»¬æ¥**å¼€ç®± PyTorch çš„ `torch.nn` æ¨¡å—**ï¼Œçœ‹çœ‹è¿™äº›â€œç‚¼ä¸¹ææ–™â€åˆ°åº•éƒ½æ˜¯å¹²å˜›çš„ã€‚

æˆ‘ä¼šæŒ‘å‡ºæœ€å¸¸ç”¨çš„å‡ ä¸ªæ¨¡å—ï¼ˆå‡½æ•°/ç±»ï¼‰ï¼Œç”¨**æç¬‘ + å®æˆ˜**çš„æ–¹å¼è®²æ¸…æ¥šæ¯ä¸€ä¸ªç”¨é€”ï¼Œå¸®ä½ åœ¨â€œç‚¼ä¸¹ç‚‰â€é‡Œä¸ç¿»è½¦ï¼

------

## `torch.nn` 

`torch.nn` å°±æ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œç§¯æœ¨ç®±å­ã€‚
 ä½ æƒ³ç›–æˆ¿å­ï¼Œå®ƒç»™ä½ æä¾›ç –å¤´ï¼ˆå±‚ï¼‰ã€ç ‚æµ†ï¼ˆæ¿€æ´»å‡½æ•°ï¼‰ã€åœ°åŸºï¼ˆæŸå¤±å‡½æ•°ï¼‰ç­‰ä¸€åº”ä¿±å…¨ã€‚

------

### ğŸ å¸¸ç”¨ `torch.nn` æ¨¡å—è¯¦è§£ï¼ˆé‡ç‚¹ï¼‰

ğŸ”¸ `nn.Linear(in_features, out_features)`

ğŸ‘‰ **å…¨è¿æ¥å±‚**ï¼Œå°±æ˜¯æœ€æ™®é€šçš„ç¥ç»å…ƒè¿æ¥å±‚ã€‚

> æŠŠä¸€å¨å‘é‡ä» A ç©ºé—´æ¬åˆ° B ç©ºé—´ï¼Œè¿˜é™„èµ æƒé‡çŸ©é˜µå’Œåç½®é¡¹ã€‚

```python
nn.Linear(128, 64)
```

è¡¨ç¤ºè¾“å…¥æ˜¯é•¿åº¦ä¸º 128 çš„å‘é‡ï¼Œè¾“å‡ºå˜æˆ 64 ç»´ã€‚

------

#### ğŸ”¸ `nn.Conv2d(in_channels, out_channels, kernel_size)`

ğŸ‘‰ **å·ç§¯å±‚**ï¼Œç”¨äºå›¾åƒç­‰ç½‘æ ¼æ•°æ®ã€‚

> ç±»ä¼¼ç”¨â€œæ»¤é•œâ€æ‰«æå›¾åƒï¼ŒæŠŠä¿¡æ¯æç‚¼å‡ºæ¥ã€‚

```python
nn.Conv2d(3, 16, kernel_size=3, padding=1)
```

è¡¨ç¤ºï¼šè¾“å…¥æ˜¯ 3 é€šé“å›¾åƒï¼ˆRGBï¼‰ï¼Œè¾“å‡º 16 ä¸ª feature mapï¼Œå·ç§¯æ ¸æ˜¯ 3Ã—3ã€‚

------

#### ğŸ”¸ `nn.ReLU()` / `nn.Tanh()` / `nn.Sigmoid()`

ğŸ‘‰ **æ¿€æ´»å‡½æ•°**ï¼Œè´Ÿè´£â€œåŠ ç‚¹è°ƒæ–™â€ï¼Œè®©ç½‘ç»œæ›´æœ‰è¡¨è¾¾åŠ›ã€‚

| å‡½æ•°        | ç‰¹ç‚¹                        |
| ----------- | --------------------------- |
| `ReLU()`    | å¤§äº0ä¿ç•™ï¼Œå°äº0å˜0ï¼ˆå¿«ï¼ï¼‰ |
| `Tanh()`    | è¾“å‡ºåœ¨ [-1, 1]ï¼ˆåƒæŒ¤å‹æœºï¼‰  |
| `Sigmoid()` | è¾“å‡ºåœ¨ [0, 1]ï¼ˆåƒæ¦‚ç‡ï¼‰     |

------

#### ğŸ”¸ `nn.MaxPool2d(kernel_size)`

ğŸ‘‰ **æ± åŒ–å±‚**ï¼Œå‡å°‘å°ºå¯¸ï¼Œä¿ç•™æœ€é‡è¦ä¿¡æ¯ã€‚

> æŠŠå±€éƒ¨å›¾åƒæµ“ç¼©æˆä¸€ä¸ªâ€œä»£è¡¨â€ã€‚å¸¸ç”¨äºé™ç»´ + æé«˜è®¡ç®—æ•ˆç‡ã€‚

```python
nn.MaxPool2d(2)
```

è¡¨ç¤º 2Ã—2 çš„åŒºåŸŸé‡Œåªä¿ç•™æœ€å¤§å€¼ã€‚

------

ğŸ”¸ `nn.Dropout(p)`

ğŸ‘‰ **é˜²æ­¢è¿‡æ‹Ÿåˆ**ï¼Œéšæœºâ€œæ–­ç”µâ€ã€‚

> åƒåœ¨è€ƒè¯•æ—¶çªç„¶ææ‰ä½ ä¸€éƒ¨åˆ†çŸ¥è¯†ç‚¹ï¼Œè®©ä½ ä¸è¦é è®°å¿†ï¼Œè€Œé ç†è§£ï¼

```python
nn.Dropout(0.5)
```

è®­ç»ƒæ—¶éšæœºä¸¢å¼ƒ 50% çš„ç¥ç»å…ƒï¼ˆæ¨ç†æ—¶ä¸ä¸¢ï¼‰ã€‚

------

#### ğŸ”¸ `nn.Flatten()`

ğŸ‘‰ æŠŠå¤šç»´å¼ é‡å±•å¼€æˆå‘é‡ï¼Œç”¨äºå·ç§¯â†’å…¨è¿æ¥çš„è¿‡æ¸¡ã€‚

```python
nn.Flatten()
```

æ¯”å¦‚ `[batch, 16, 4, 4]` â†’ `[batch, 256]`

------

#### ğŸ”¸ `nn.CrossEntropyLoss()`

ğŸ‘‰ **åˆ†ç±»ä»»åŠ¡çš„æŸå¤±å‡½æ•°**ï¼ˆè‡ªåŠ¨ softmax + log lossï¼‰

> å®ƒè´Ÿè´£è¯„ä¼°ï¼šâ€œæ¨¡å‹è¾“å‡ºå¾—å¤šç¦»è°±ï¼Ÿâ€

```python
loss_fn = nn.CrossEntropyLoss()
loss = loss_fn(outputs, labels)
```

------

#### ğŸ§± ç¤ºä¾‹ä»£ç ç»„åˆï¼š

ä¸‹é¢è¿™æ®µä»£ç å±•ç¤ºäº†å¸¸ç”¨æ¨¡å—å¦‚ä½•ä¸€èµ·ç”¨ï¼š

```python
import torch.nn as nn

model = nn.Sequential(
    nn.Conv2d(1, 6, 5),
    nn.ReLU(),
    nn.MaxPool2d(2),
    nn.Conv2d(6, 16, 5),
    nn.ReLU(),
    nn.MaxPool2d(2),
    nn.Flatten(),
    nn.Linear(16*4*4, 120),
    nn.ReLU(),
    nn.Linear(120, 84),
    nn.ReLU(),
    nn.Linear(84, 10)
)
```

è¿™å°±æ˜¯ä¸€ä¸ªæ ‡å‡†çš„ LeNetï¼

------

#### ğŸ¯ æ€»ç»“ï¼šç‚¼ä¸¹å£è¯€

| æ¨¡å—               | å¹²å•¥çš„                         |
| ------------------ | ------------------------------ |
| `Conv2d`           | æ»¤å›¾ä¸“å®¶ï¼ˆæå–ç‰¹å¾ï¼‰           |
| `MaxPool2d`        | å›¾åƒå‹ç¼©ï¼ˆä¿ç•™ç²¾åï¼‰           |
| `Linear`           | ç¥ç»å…ƒé“¾æ¥ï¼ˆåƒåŠå…¬å®¤æ¬æ¡Œå­ï¼‰   |
| `ReLU`             | éçº¿æ€§è°ƒæ–™åŒ…                   |
| `Dropout`          | é˜²ä½œå¼Šæœºåˆ¶                     |
| `Flatten`          | æ‹æ‰æ‰€æœ‰ç‰¹å¾æ–¹ä¾¿é€è¿›å…¨è¿æ¥     |
| `CrossEntropyLoss` | çœ‹æ¨¡å‹å‡ºé”™å¤šå°‘ï¼Œå‘Šè¯‰å®ƒè¯¥æ€ä¹ˆæ”¹ |

# æ„Ÿå—é‡

[[å½»åº•ææ‡‚æ„Ÿå—é‡çš„å«ä¹‰ä¸è®¡ç®—](https://www.cnblogs.com/shine-lee/p/12069176.html)]

![https://www.researchgate.net/publication/316950618_Maritime_Semantic_Labeling_of_Optical_Remote_Sens](https://s2.ax1x.com/2019/12/16/Q4wXPf.png)

| åè¯                      | æ„æ€                                         | ä¸¾ä¾‹                                    |
| ------------------------- | -------------------------------------------- | --------------------------------------- |
| æ„Ÿå—é‡ï¼ˆreceptive fieldï¼‰ | å½“å‰ç¥ç»å…ƒåœ¨è¾“å…¥å›¾åƒä¸Šæ‰€å¯¹åº”çš„â€œå½±å“åŒºåŸŸâ€     | Conv3 ä¸­ä¸€ä¸ªç¥ç»å…ƒå¯¹åº”åŸå›¾çš„ 15Ã—15 åŒºåŸŸ |
| æ„Ÿå—é‡ä¸­å¿ƒè·ç¦»            | é‚»è¿‘ä¸¤ä¸ªç¥ç»å…ƒçš„æ„Ÿå—é‡ä¸­å¿ƒåœ¨è¾“å…¥å›¾åƒä¸­çš„é—´è· | = stride                                |
| æ„Ÿå—é‡è¶Šå¤§                | è¡¨ç¤ºç¥ç»å…ƒèƒ½â€œçœ‹åˆ°â€æ›´å¤§çš„å›¾åƒèŒƒå›´             | æ›´æœ‰åˆ©äºç†è§£å…¨å±€ç»“æ„                    |

æ„Ÿå—é‡å¤§å°=ä¸Šä¸€å±‚æ„Ÿå—é‡å¤§å°+(æœ¬å±‚å·ç§¯æ ¸å¤§å°âˆ’1)Ã—ä¸Šä¸€å±‚æ€»æ­¥é•¿

# æ­£åˆ™åŒ–

> **é˜²æ­¢æ¨¡å‹â€œè¿‡æ‹Ÿåˆâ€è®­ç»ƒæ•°æ®ï¼Œè®©æ¨¡å‹æ³›åŒ–èƒ½åŠ›æ›´å¼ºï¼**

> **æ­£åˆ™åŒ–å°±æ˜¯â€œç»™æ¨¡å‹åŠ ç‚¹çº¦æŸâ€ï¼Œè®©å®ƒä¸è¦å­¦å¾—å¤ªå¤æ‚ã€‚**

æ¯”å¦‚ä½ è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå®ƒåœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å®Œç¾ï¼Œä½†åœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°å¾ˆå·®ï¼Œè¿™å°±æ˜¯**è¿‡æ‹Ÿåˆ**äº†ã€‚
 æ­¤æ—¶æˆ‘ä»¬å°±ç”¨**æ­£åˆ™åŒ–æŠ€æœ¯**ï¼Œæ¥**è®©æ¨¡å‹å­¦å¾—â€œä¸è¿‡äºå¤æ‚â€**ï¼Œä»è€Œæå‡å®ƒåœ¨æ–°æ•°æ®ä¸Šçš„è¡¨ç°ã€‚

------

## ğŸ¯ å¸¸è§æ­£åˆ™åŒ–æ–¹æ³•ï¼ˆæŒ‰ç±»å‹åˆ†ï¼‰

| æ­£åˆ™åŒ–æ–¹æ³•                     | åŸç†                         | ç±»æ¯”                         | ä¸¾ä¾‹                  |
| ------------------------------ | ---------------------------- | ---------------------------- | --------------------- |
| **L1/L2 æ­£åˆ™**                 | é™åˆ¶æƒé‡å¤§å°ï¼ˆæƒ©ç½šå¤§æƒé‡ï¼‰   | ç»™ç¥ç»ç½‘ç»œâ€œä¸Šç´§ç®å’’â€         | Ridge/Lasso å›å½’ï¼ŒCNN |
| **Dropout**                    | éšæœºâ€œä¸¢å¼ƒâ€ä¸€éƒ¨åˆ†ç¥ç»å…ƒ       | ä¸Šè¯¾éšæœºç‚¹åï¼Œä¸èƒ½åªé æŸäº›äºº | CNNã€RNNã€Transformer |
| **æ•°æ®å¢å¼º**                   | æŠŠå›¾ç‰‡ç¿»è½¬/æ—‹è½¬/è£å‰ª         | å¤šç»™æ¨¡å‹ä¸åŒè§†è§’             | å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹    |
| **æå‰åœæ­¢ï¼ˆEarly Stoppingï¼‰** | éªŒè¯é›†æŸå¤±ä¸å†ä¸‹é™å°±åœæ­¢è®­ç»ƒ | ä¸ç†¬å¤œå¤ä¹ åˆ°åï¼Œä¸å¦‚æ—©ç‚¹ç¡   | æ‰€æœ‰æ¨¡å‹éƒ½é€‚ç”¨        |
| **Batch Normalization**        | ç¨³å®šä¸­é—´å±‚åˆ†å¸ƒ               | ç»™æ¯å±‚â€œè°ƒå¹³è¡¡â€               | CNNã€Transformer      |

------

## ğŸ”§ æ­£åˆ™åŒ–çš„å…¬å¼åŸç†ï¼ˆä»¥ L2 ä¸ºä¾‹ï¼‰

å‡è®¾ä½ çš„æŸå¤±å‡½æ•°æ˜¯ï¼š

```
Loss = åŸå§‹æŸå¤±ï¼ˆå¦‚äº¤å‰ç†µï¼‰ + Î» Ã— ||W||Â²
```

- `||W||Â²` è¡¨ç¤ºæ‰€æœ‰æƒé‡çš„å¹³æ–¹å’Œï¼ˆè¶Šå¤§æƒ©ç½šè¶Šå¤§ï¼‰
- `Î»` æ˜¯æ­£åˆ™åŒ–ç³»æ•°ï¼Œæ§åˆ¶æƒ©ç½šå¼ºåº¦
- ç›®æ ‡æ˜¯è®©å‚æ•° W ä¸è¦å¤ªå¤§ â†’ ç½‘ç»œä¸ä¼šå¤ªå¤æ‚

| æ¨¡å‹ç±»å‹             | è®­ç»ƒé›†è¡¨ç° | æµ‹è¯•é›†è¡¨ç° | è¯´æ˜     |
| -------------------- | ---------- | ---------- | -------- |
| è¿‡æ‹Ÿåˆæ¨¡å‹           | âœ… å¾ˆå¥½     | âŒ å¾ˆå·®     | å­¦å¤ªæ­»   |
| æ¬ æ‹Ÿåˆæ¨¡å‹           | âŒ å¾ˆå·®     | âŒ å¾ˆå·®     | å­¦å¤ªæµ…   |
| æ­£å¸¸æ¨¡å‹ï¼ˆ+ æ­£åˆ™åŒ–ï¼‰ | âœ… ä¸é”™     | âœ… ä¸é”™     | å­¦å¾—åˆšå¥½ |

------

## ä½¿ç”¨åœºæ™¯

| åœºæ™¯                     | æ¨èç”¨æ³•               |
| ------------------------ | ---------------------- |
| æ¨¡å‹å¤ªå¤æ‚ / å±‚æ•°å¤š      | L2 æ­£åˆ™ / Dropout / BN |
| è®­ç»ƒæ•°æ®å¤ªå°‘             | æ•°æ®å¢å¼º / Dropout     |
| è®­ç»ƒ loss å¾ˆä½ä½†æµ‹è¯•å¾ˆçƒ‚ | ç”¨æ­£åˆ™åŒ–é˜²è¿‡æ‹Ÿåˆï¼     |

------

## âœ… æ€»ç»“2

| å…³é”®è¯   | å«ä¹‰                                    |
| -------- | --------------------------------------- |
| æ­£åˆ™åŒ–   | é™åˆ¶æ¨¡å‹å¤æ‚åº¦ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ              |
| å¸¸ç”¨æ–¹æ³• | L1/L2ã€Dropoutã€æ•°æ®å¢å¼ºã€EarlyStopping |
| å¥½å¤„     | æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œè®©å®ƒå­¦å¾—â€œåˆšåˆšå¥½â€    |

------

# è¿‡æ‹Ÿåˆ

[è¿‡æ‹Ÿåˆ](https://medium.com/@mangeshsalunke1309/overfitting-in-neural-networks-9dd5f26370c0)

## ä»€ä¹ˆæ˜¯è¿‡åº¦æ‹Ÿåˆï¼Ÿ

å½“ç¥ç»ç½‘ç»œä¸ä»…å­¦ä¹ æ•°æ®ä¸­çš„ä¸€èˆ¬æ¨¡å¼ï¼Œè¿˜å­¦ä¹ è®­ç»ƒæ•°æ®é›†ä¸­ç‰¹æœ‰çš„å™ªå£°å’Œç»†å¾®ç»†èŠ‚æ—¶ï¼Œå°±ä¼šå‘ç”Ÿè¿‡æ‹Ÿåˆã€‚è¿™ä¼šå¯¼è‡´æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨æ–°çš„ã€æœªçŸ¥çš„æ•°æ®ä¸Šå´è¡¨ç°ä¸ä½³ã€‚ä¸ºä»€ä¹ˆç¥ç»ç½‘ç»œä¼šè¿‡æ‹Ÿåˆï¼Ÿ

## ä¸ºä»€ä¹ˆç¥ç»ç½‘ç»œä¼šè¿‡åº¦æ‹Ÿåˆï¼Ÿ

1. åŸå› ä¼—å¤šï¼Œå…¶ä¸­ä¸€ä¸ªåŸå› æ˜¯**â€œç¥ç»ç½‘ç»œçš„å¤æ‚æ¶æ„â€**ã€‚å½“æˆ‘ä»¬åœ¨ç¥ç»ç½‘ç»œçš„è¯­å¢ƒä¸­è°ˆè®ºå¤æ‚æ¶æ„æ—¶ï¼Œæˆ‘ä»¬æŒ‡çš„æ˜¯å…·æœ‰å¤šå±‚ï¼ˆæ·±åº¦ç½‘ç»œï¼‰å’Œå¤§é‡å‚æ•°ï¼ˆæƒé‡å’Œåå·®ï¼‰çš„æ¨¡å‹ã€‚å¤æ‚ç½‘ç»œæ‹¥æœ‰è¶³å¤Ÿçš„å®¹é‡æ¥è®°å¿†è®­ç»ƒæ•°æ®ä¸­çš„æ¯ä¸ªç»†èŠ‚ï¼ŒåŒ…æ‹¬å™ªå£°ã€å¼‚å¸¸æˆ–ç¦»ç¾¤å€¼ã€‚è™½ç„¶è¿™ç¡®ä¿äº†è®­ç»ƒé›†çš„é«˜ç²¾åº¦ï¼Œä½†å®ƒæ— æ³•æ¨å¹¿åˆ°æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ã€‚
2. å¦‚æœæˆ‘ä»¬æ‹¥æœ‰å¤æ‚çš„ç¥ç»ç½‘ç»œæ¶æ„å’Œ**å°‘é‡æ•°æ®**ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿæ˜¯çš„ï¼Œå³ä½¿è¿™æ ·ä¹Ÿå¯èƒ½å¯¼è‡´è¿‡åº¦æ‹Ÿåˆã€‚ç¥ç»ç½‘ç»œæ‹¥æœ‰å¤§é‡å¯è®­ç»ƒçš„å‚æ•°ï¼ˆæƒé‡å’Œåå·®ï¼‰ã€‚å°å‹æ•°æ®é›†æ— æ³•æä¾›è¶³å¤Ÿçš„ä¿¡æ¯æ¥æ­£ç¡®çº¦æŸè¿™äº›å‚æ•°ï¼Œå¯¼è‡´ç½‘ç»œâ€œè®°å¿†â€æ•°æ®è€Œä¸æ˜¯å­¦ä¹ ä¸€èˆ¬æ¨¡å¼ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æ­£åœ¨è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œè¯†åˆ«åŠ¨ç‰©ï¼Œä½†åªæœ‰ 10 å¼ çŒ«ç‹—å›¾åƒï¼Œé‚£ä¹ˆç½‘ç»œå¯èƒ½ä¼šæ‹¾å–ä¸ç›¸å…³çš„æ¨¡å¼ï¼ˆä¾‹å¦‚èƒŒæ™¯ã€å…‰çº¿æˆ–åŠ¨ç‰©çš„ç‰¹å®šå§¿åŠ¿ï¼‰ï¼Œè€Œä¸æ˜¯å¯¹æ‰€æœ‰çŒ«ç‹—è¿›è¡Œæ³›åŒ–ã€‚
3. å¯¹æ¨¡å‹è¿›è¡Œè¿‡å¤šçš„è®­ç»ƒå‘¨æœŸä¼šå¯¼è‡´ç½‘ç»œå­¦ä¹ åˆ°è®­ç»ƒæ•°æ®ä¸­çš„å™ªå£°å’Œå¼‚å¸¸å€¼ã€‚è¿™ç§**æ¨¡å‹è¿‡åº¦è®­ç»ƒ**ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼ŒåŒæ—¶ä¹Ÿä¼šæµªè´¹æˆ‘ä»¬å®è´µçš„è®¡ç®—èµ„æºã€‚
4. è®©æˆ‘ä»¬æ¥è°ˆè°ˆ**é«˜å­¦ä¹ ç‡**ï¼Œä»¥åŠå¦‚æœè®¾ç½®ä¸å½“ï¼Œå®ƒä¼šå¦‚ä½•æŠŠäº‹æƒ…æç ¸ï¼å­¦ä¹ ç‡æ§åˆ¶ç€ç¥ç»ç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¯ä¸€æ­¥è°ƒæ•´æƒé‡çš„å¹…åº¦ã€‚å¦‚æœå­¦ä¹ ç‡è¿‡é«˜ï¼Œæ¨¡å‹åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ä¼šé‡‡å–è¿‡å¤§çš„æ­¥é•¿ã€‚å®ƒä¸ä¼šé€æ¸è¶‹å‘æœ€ä½³æƒé‡ï¼Œè€Œæ˜¯ä¸æ–­è¶…è°ƒï¼Œåœ¨æœ€ä¼˜å€¼é™„è¿‘åå¤æ³¢åŠ¨ï¼Œæœ€ç»ˆæ— æ³•çœŸæ­£è¾¾åˆ°æœ€ä¼˜å€¼ã€‚åœ¨è¿™ä¸ªæ··ä¹±çš„è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹æœ€ç»ˆå¯èƒ½ä»ç„¶èƒ½å¤Ÿç´§å¯†æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚ä½†ç”±äºå®ƒè¿˜æ²¡æœ‰çœŸæ­£å½¢æˆä¸€ä¸ªç¨³å®šçš„æ¨¡å¼æ³›åŒ–ï¼Œå› æ­¤å®ƒåœ¨æ–°çš„ã€æœªçŸ¥çš„æ•°æ®ä¸Šè¡¨ç°ä¸ä½³ã€‚

# æ¬ æ‹Ÿåˆ

## ä»€ä¹ˆæ˜¯æ¬ æ‹Ÿåˆï¼Ÿ

æ¬ æ‹Ÿåˆæ˜¯æŒ‡æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œæ— æ³•æ­£ç¡®æ˜ å°„è¾“å…¥å’Œè¾“å‡ºä¹‹é—´å…³ç³»çš„æƒ…å†µã€‚ç¥ç»ç½‘ç»œåœ¨è®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†ä¸Šçš„è¡¨ç°éƒ½ä¸ä½³ã€‚ï¼ˆè®°ä½ï¼Œåœ¨è¿‡æ‹Ÿåˆä¸­ï¼Œæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šå´å¤±è´¥äº†ï¼‰ã€‚æ¬ æ‹Ÿåˆæ˜¯æŒ‡ç¥ç»ç½‘ç»œç”±äºæ•°æ®é›†è¿‡äºç®€å•ï¼Œæ— æ³•æŒæ¡å…¶ä¸­å¤æ‚çš„æ¨¡å¼å’Œè¶‹åŠ¿ã€‚

## ä¸ºä»€ä¹ˆç¥ç»ç½‘ç»œä¸é€‚åˆï¼Ÿ

1. ä¸ºå¤æ‚æ•°æ®æ„å»º**ç®€å•çš„ç¥ç»ç½‘ç»œ**æ˜¯å¯¼è‡´æ¬ æ‹Ÿåˆçš„ä¸»è¦åŸå› ã€‚ç®€å•çš„ç¥ç»ç½‘ç»œæ„å‘³ç€æˆ‘ä»¬ä½¿ç”¨çš„å±‚æ•°è¾ƒå°‘ï¼Œæˆ–æ¯å±‚çš„ç¥ç»å…ƒæ•°é‡ä¸è¶³ï¼Œæˆ–ä¸¤è€…å…¼è€Œæœ‰ä¹‹ã€‚å½“é—®é¢˜é™ˆè¿°å®é™…ä¸Šéœ€è¦éçº¿æ€§å†³ç­–è¾¹ç•Œæ—¶ï¼Œæˆ‘ä»¬æœ‰æ—¶ä¼šä½¿ç”¨çº¿æ€§æ¿€æ´»å‡½æ•°ã€‚
2. æœ‰æ—¶ç”±äºè®¡ç®—èµ„æºä¸è¶³ï¼Œæˆ‘ä»¬å€¾å‘äºè®­ç»ƒæ¨¡å‹è¾ƒå°‘çš„epochæ•°ã€‚è¿™ä¼šå¯¼è‡´**ç¥ç»ç½‘ç»œè®­ç»ƒä¸å®Œå…¨ï¼Œ**å³åœ¨ç½‘ç»œå°šæœªç¡®å®šå¯å­¦ä¹ å‚æ•°ï¼ˆä¾‹å¦‚æƒé‡å’Œåå·®ï¼‰æ—¶å°±è¿‡æ—©åœæ­¢è®­ç»ƒã€‚è¿™å¯èƒ½å¯¼è‡´ç¥ç»ç½‘ç»œæ¬ æ‹Ÿåˆï¼Œæ€§èƒ½ä¸‹é™ã€‚
3. æˆ‘ä»¬æä¾›ç»™ç¥ç»ç½‘ç»œçš„æ•°æ®ä¸å¤Ÿä¸°å¯Œæˆ–æœ‰ç”¨ï¼Œä¸è¶³ä»¥å¸®åŠ©å®ƒå¾ˆå¥½åœ°å­¦ä¹ ã€‚å¦‚æœ**æ•°æ®ç¼ºä¹é‡è¦ç»†èŠ‚**æˆ–å……æ–¥ç€ä¸å¿…è¦çš„ä¿¡æ¯ï¼Œæ¨¡å‹å°±å¾ˆéš¾æ‰¾åˆ°æœ‰æ„ä¹‰çš„æ¨¡å¼ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œä½ æ­£åœ¨è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œæ¥é¢„æµ‹æˆ¿ä»·ã€‚å¦‚æœæ•°æ®é›†ä»…åŒ…å«æˆ¿å±‹çš„é¢œè‰²å’Œå‰é—¨ç±»å‹ï¼Œè€Œç¼ºå°‘æˆ¿å±‹å¤§å°ã€å§å®¤æ•°é‡æˆ–ä½ç½®ç­‰å…³é”®ç»†èŠ‚ï¼Œæ¨¡å‹çš„è¡¨ç°å°±ä¼šå¾ˆå·®ã€‚è¿™å°±åƒä»…å‡­æ²¹æ¼†é¢œè‰²æ¥çŒœæµ‹æ±½è½¦çš„ä»·æ ¼ä¸€æ ·ï¼

## **å¦‚ä½•é¿å…æ¬ æ‹Ÿåˆï¼Ÿ**

1. è§£å†³æ¬ æ‹Ÿåˆæœ€åŸºæœ¬çš„æ–¹æ³•æ˜¯**å¢åŠ ç¥ç»ç½‘ç»œçš„å¤æ‚åº¦**ã€‚å¦‚ä½•è®©å®ƒæ›´å¤æ‚ï¼Ÿå¢åŠ å±‚æ•°ï¼Œå¢åŠ æ¯å±‚çš„ç¥ç»å…ƒæ•°é‡ã€‚ä½¿ç”¨ ReLUã€Tanh ç­‰æ¿€æ´»å‡½æ•°ï¼Œè€Œä¸æ˜¯ç®€å•çš„çº¿æ€§æ¿€æ´»å‡½æ•°ã€‚
2. å°è¯•**è®­ç»ƒç¥ç»ç½‘ç»œæ›´å¤šè½®æ¬¡**ã€‚å°è¯•ä¸åŒçš„è½®æ¬¡æ•°é‡ï¼Œç›´åˆ°æ¨¡å‹è®­ç»ƒå……åˆ†ã€‚å°è¯•ä¸åŒçš„å­¦ä¹ ç‡å€¼ã€‚
3. **é¿å…è¿‡åº¦ä½¿ç”¨æ­£åˆ™åŒ–æŠ€æœ¯**ï¼Œä¾‹å¦‚ L1ã€L2ã€Dropouts ç­‰ã€‚å³ä½¿æˆ‘ä»¬æƒ³è¿™æ ·åšï¼Œä¹Ÿè¦é™ä½ Dropout ç‡æˆ–é™ä½ L1ã€L2 æ­£åˆ™åŒ–çš„å¼ºåº¦ã€‚è¿™å¯ä»¥ç¡®ä¿ç¥ç»ç½‘ç»œä¿æŒå…¶å¤æ‚æ€§ã€‚
4. å¦‚æœæ•°æ®é‡ä¸è¶³ä»¥è¦†ç›–é—®é¢˜åŸŸçš„æ‰€æœ‰ç¤ºä¾‹æ¡ˆä¾‹ï¼Œåˆ™å¢åŠ **æ•°æ®é‡ã€‚**





# Codingä½œä¸š1

Run the training and testing of Faster RCNN on Pascal VOC 2007&2012

â€¢The code  https://github.com/open-mmlab/mmdetection
This code is built by the popular object detection tool â€˜mmdetectionâ€™, which includes most of recent detection models and is easy to run.

è¿™ä¸ªé…ç½®ç¯å¢ƒçš„è¯æ˜¯æŒ‰ç…§openmmlabçš„æ•™ç¨‹è¿›è¡Œé…ç½®çš„ï¼Œåœ¨æ•™ç¨‹ä¸Šä¼šæœ‰æµ‹è¯•é…ç½®æ˜¯å¦æˆåŠŸçš„ç¯èŠ‚

[æ¦‚è¿°](https://mmdetection.readthedocs.io/zh_CN/latest/get_started.html)å¯¹ MMDetection è¿›è¡Œåˆæ­¥çš„äº†è§£ã€‚

â€¢Download the code and build the running environment.

â€¢Download the dataset and pre-trained model.

ä¸‹è½½æ•°æ®é›†åœ¨å®˜ç½‘ä¸Šä¹Ÿæœ‰

ä¸€äº›ç¤ºä¾‹

```
python tools/misc/download_dataset.py --dataset-name coco2017
python tools/misc/download_dataset.py --dataset-name voc2007  
python tools/misc/download_dataset.py --dataset-name lvis
```

å› ä¸ºè¿™é‡Œè¦ä¸‹è½½Pascal VOC 2007&2012

æ‰€ä»¥ä»£ç æ˜¯ï¼ˆä¸èƒ½åŒæ—¶ä¸‹è½½ä¸¤ä¸ªï¼‰

```
python tools/misc/download_dataset.py --dataset-name voc2007 --save-dir ./data/VOCdevkit/VOC2007 --unzip
python tools/misc/download_dataset.py --dataset-name voc2012 --save-dir ./data/VOCdevkit/VOC2012 --unzip
```

éœ€è¦æ‰‹åŠ¨è°ƒæ•´ä¸€ä¸‹æ–‡ä»¶ç»“æ„

```
data/
â””â”€â”€ VOCdevkit/
    â”œâ”€â”€ VOC2007/
    â”‚   â”œâ”€â”€ JPEGImages/
    â”‚   â”œâ”€â”€ Annotations/
    â”‚   â””â”€â”€ ImageSets/Main/
    â””â”€â”€ VOC2012/
        â”œâ”€â”€ JPEGImages/
        â”œâ”€â”€ Annotations/
        â””â”€â”€ ImageSets/Main/
```

ä¸€äº›å‚æ•°

![image-20250623193050748](C:\Users\Anna\AppData\Roaming\Typora\typora-user-images\image-20250623193050748.png)

ä¸‹è½½é…ç½®æ–‡ä»¶å’Œæ¨¡å‹æƒé‡æ–‡ä»¶

```
mim download mmdet --config faster-rcnn_r50_fpn_1x_coco --dest ./checkpoints
```

æ³¨æ„è¿™é‡Œåº”è¯¥æ˜¯å°±æ˜¯å±äºä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¿™é‡Œ**é¢„è®­ç»ƒæ¨¡å‹**æ˜¯æŒ‡å·²ç»åœ¨å¤§å‹æ•°æ®é›†ï¼ˆæ¯”å¦‚ COCOï¼‰ä¸Šè®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°ï¼Œç”¨æ¥ä½œä¸ºä½ è®­ç»ƒ VOC æ•°æ®çš„èµ·ç‚¹ï¼Œå¯ä»¥æå‡è®­ç»ƒé€Ÿåº¦å’Œå‡†ç¡®ç‡ã€‚

å¤§éƒ¨åˆ†éƒ½æ˜¯åœ¨cocoä¸Šçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä¹Ÿå°±æ˜¯è¯´faster-rcnn_r50_fpn_1x_voc0712æ˜¯æ²¡æœ‰çš„

â€¢Check the tutorial.

â€¢Run training with configs/pascal_voc/faster_rcnn_r50_fpn_1x_voc0712.py

```
python tools/train.py configs/pascal_voc/faster-rcnn_r50_fpn_1x_voc0712.py --work-dir work_dirs/voc0712_faster_rcnn
```

â€¢Test the model.

```
python tools/test.py <é…ç½®æ–‡ä»¶è·¯å¾„> <æ¨¡å‹æƒé‡è·¯å¾„> --eval mAP
```

```
python tools/test.py configs/pascal_voc/faster-rcnn_r50_fpn_1x_voc0712.py work_dirs /voc0712_faster_rcnn/latest.pth --eval mAP
```

# MobileNet EfficientNet ç½‘ç»œåŠ é€Ÿä¸»è¦æœ‰å“ªäº›æ–¹å¼ï¼Ÿ

![1ccca66c46126ee1525b6e6d8dcf6cb](D:\WeChatfiles\WeChat Files\wxid_lqrkt1mxadlp31\FileStorage\Temp\1ccca66c46126ee1525b6e6d8dcf6cb.jpg)

![5fd4d24708a152ed349a01a51545bc0](D:\WeChatfiles\WeChat Files\wxid_lqrkt1mxadlp31\FileStorage\Temp\5fd4d24708a152ed349a01a51545bc0.jpg)

![0dd06d70219aa31ba70d024891d5657](D:\WeChatfiles\WeChat Files\wxid_lqrkt1mxadlp31\FileStorage\Temp\0dd06d70219aa31ba70d024891d5657.jpg)

![029dc817d1f255631c88d4b3011f7d9](D:\WeChatfiles\WeChat Files\wxid_lqrkt1mxadlp31\FileStorage\Temp\029dc817d1f255631c88d4b3011f7d9.jpg)



![d2dacd64d40577a019663da3664746b](D:\WeChatfiles\WeChat Files\wxid_lqrkt1mxadlp31\FileStorage\Temp\d2dacd64d40577a019663da3664746b.jpg)

![7f3027ce86a708f5a97a82489d9df90](D:\WeChatfiles\WeChat Files\wxid_lqrkt1mxadlp31\FileStorage\Temp\7f3027ce86a708f5a97a82489d9df90.jpg)





# Mask2fomerè®­ç»ƒ

å®Œç¾ï¼ğŸ‰ ä½ å·²ç»æˆåŠŸæ¿€æ´»äº†è™šæ‹Ÿç¯å¢ƒ `(mask2former_venv)`ï¼Œæˆ‘ä»¬ç°åœ¨æ­£å¼è¿›å…¥è®­ç»ƒ Mask2Former çš„å®æˆ˜é˜¶æ®µï¼Œç›®æ ‡æ˜¯ï¼š

> âœ… å®‰è£… Mask2Former ä¾èµ– â†’ âœ… ä¸‹è½½ ADE20K æ•°æ® â†’ âœ… å¯åŠ¨è®­ç»ƒï¼

------

## âœ… æ­¥éª¤ä¸€ï¼šå®‰è£…ä¾èµ–ï¼ˆå¤åˆ¶ä¸‹é¢è¿™ç»„å‘½ä»¤ä¸€æ¬¡è¿è¡Œï¼‰

```bash
# å‡çº§ pip å’ŒåŸºç¡€åŒ…
pip install --upgrade pip setuptools wheel

# å®‰è£… PyTorchï¼ˆä½ å·²æœ‰ torch2.3.1ï¼Œå¯è·³è¿‡ï¼Œä½†ä¿é™©èµ·è§é‡è£…ä¹Ÿæ²¡é—®é¢˜ï¼‰
pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121

# å®‰è£… Detectron2ï¼ˆå®˜æ–¹åº“ï¼Œå¿…é¡»æœ‰ï¼‰
pip install 'git+https://github.com/facebookresearch/detectron2.git'

# å®‰è£…å…¶ä»–å¸¸ç”¨ä¾èµ–
pip install opencv-python pycocotools scipy yacs matplotlib tqdm tensorboard

# å…‹éš† Mask2Former æºç 
git clone https://github.com/facebookresearch/Mask2Former.git
cd Mask2Former

# å®‰è£…é¡¹ç›®ä¾èµ–ï¼ˆè¿™æ˜¯å…³é”®æ­¥éª¤ï¼‰
pip install -e .
```

å¦‚æœä½ çœ‹åˆ°æœ€åä¸€å¥è¾“å‡ºç±»ä¼¼ï¼š

```
Successfully installed mask2former-0.1
```

è¯´æ˜ä½ å·²ç»å®Œå…¨å°±ç»ªã€‚

------

## âœ… æ­¥éª¤äºŒï¼šä¸‹è½½æ•°æ®é›†ï¼ˆæˆ‘ä»¬å…ˆç”¨è½»é‡ç‰ˆ ADE20Kï¼‰

åœ¨ Mask2Former ç›®å½•ä¸‹è¿è¡Œï¼š

æˆ‘æ˜¯ç›´æ¥ä»kaggleä¸‹è½½ade20kçš„å‹ç¼©åŒ…

```bash
mkdir -p datasets/ADE20K
cd datasets/ADE20K

# ä¸‹è½½ ADE20K å®˜æ–¹æ•°æ®é›†ï¼ˆå¤§çº¦3.5GBï¼‰
wget http://images.cocodataset.org/zips/ADEChallengeData2016.zip

# è§£å‹æ•°æ®
unzip ADEChallengeData2016.zip
cd ../../../
```

è¿™æ­¥å®Œæˆåï¼Œä½ çš„æ•°æ®å°±ä¼šåœ¨ï¼š

```
Mask2Former/datasets/ADE20K/ADEChallengeData2016/
```

------

## âœ… æ­¥éª¤ä¸‰ï¼šå‡†å¤‡é…ç½®æ–‡ä»¶

ç»§ç»­åœ¨ `Mask2Former` æ ¹ç›®å½•ä¸­æ‰§è¡Œï¼š

```bash
mkdir -p configs/ade20k/semantic-segmentation
wget https://raw.githubusercontent.com/facebookresearch/Mask2Former/main/configs/ade20k/semantic-segmentation/swin/maskformer2_swin_large_IN21k_384_bs16_160k_res640.yaml \
-P configs/ade20k/semantic-segmentation/swin/
```

------

### ğŸ”§ ä¿®æ”¹ config æ–‡ä»¶ä¸­æ•°æ®è·¯å¾„ä¸ batch size

ç”¨ JupyterLab çš„**æ–‡æœ¬ç¼–è¾‘å™¨**æ‰“å¼€è¿™ä¸ªæ–‡ä»¶ï¼š

```
configs/ade20k/semantic-segmentation/mask2former_swin_tiny_ade20k.yaml
```

è¯·ç¡®è®¤ï¼ˆæˆ–ä¿®æ”¹ï¼‰ä»¥ä¸‹ä¸¤ä¸ªåœ°æ–¹ï¼š

#### âœ… æ•°æ®è·¯å¾„

```yaml
DATASETS:
  TRAIN: ("ade20k_sem_seg_train",)
  TEST: ("ade20k_sem_seg_val",)
INPUT:
  DATASET_MAPPER_NAME: "mask_former_semantic"
```

æ­¤å¤„æ— éœ€åŠ¨ï¼Œä½†ä½ éœ€è¦ä¿è¯ `datasets/ADE20K/ADEChallengeData2016/` ä¸­ç»“æ„æ­£ç¡®ï¼ˆç¨åæˆ‘å¸®ä½ æ£€æŸ¥ï¼‰

#### âœ… batch sizeï¼ˆé˜²æ­¢æ˜¾å­˜æº¢å‡ºï¼‰

```yaml
SOLVER:
  IMS_PER_BATCH: 2  # é»˜è®¤å¯èƒ½æ˜¯16ï¼Œæ”¹æˆ2æˆ–1
```

------

## âœ… æ­¥éª¤å››ï¼šå¼€å§‹è®­ç»ƒï¼

ç°åœ¨å¯ä»¥æ‰§è¡Œè®­ç»ƒå‘½ä»¤å•¦ï¼

```bash
python train_net.py \
  --config-file configs/ade20k/semantic-segmentation/maskformer2_swin_large_IN21k_384_bs16_160k_res640.yaml \
  --num-gpus 1 \
  OUTPUT_DIR output/ade20k_swin_tiny
```

ä½ ä¼šçœ‹åˆ°æ—¥å¿—è¾“å‡ºï¼š

- lossã€lrã€mIoUã€iter
- æ¯è½®ä¿å­˜çš„ checkpoint ä¼šä¿å­˜åœ¨ `output/ade20k_swin_tiny/`



# å¤šå¡è®­ç»ƒ

[PyTorchå¤šå¡åˆ†å¸ƒå¼è®­ç»ƒDDPå•æœºå¤šå¡ - æµ·_çº³ç™¾å· - åšå®¢å›­](https://www.cnblogs.com/chentiao/p/17666330.html)
